\section{Competing risks}
\newcommand{\Twid}{\mbox{\(\tt\sim\)}}
The competing risks routine is very general, allowing subjects
to enter or exit states multiple times.
For this reason I prefer the label ``current prevalence'' estimate, 
since it estimates what fraction of the subjects are in any
given state across time.
The easiest way to understand the estimate is to consider first the
case of no censoring.  
In that setting the estimate of $F_k(t) = 1-S_k(t)$ is the number of
subjects in state $k$ at time $t$ divided by $n$, the original
sample size.
When there is censoring the conceptually simple way to extend this
is via the redistribute-to-the-right algorithm, which allocates the
case weight for a censored subject evenly to all the others in the
same state at the time of censoring.  

The literature refers to the as ``cumulative incidence'' curves,
which is confusing, but the routine name survfitCI endures.
The cannonical call is
\begin{verbatim}
  fit <- survfit(Surv(time, status, type='mstate') ~ sex, data=mine)
\end{verbatim}
 Optionally, there can be an id statement
or cluster term to indicate a data set with multiple transitions per subject.
A multi-state survival has a status variable with multiple levels,
the first of which by default is censoring, and others indicating
the type of transition that occured.
The result will be a matrix of survival curves, one for each event type.
Subjects are assumed
to start in a "null" state, which is not tabulated for survival.
To change this behavior, give all subjects some other initial state.
  

The first part of the code is standard, parsing out options and
checking the data.
<<survfitCI>>= 
survfitCI <- function(X, Y, weights, id, istate, 
                      type=c('kaplan-meier', 'fleming-harrington', 'fh2'),
                      se.fit=TRUE,
                      conf.int= .95,
                      conf.type=c('log',  'log-log',  'plain', 'none'),
                      conf.lower=c('usual', 'peto', 'modified')){

    method <- match.arg(type)
#    error <- match.arg(error)
#    if (error != "inf")
#        warning("Only the infinetesimal jackknife error is supported for CI curves")
    conf.type <- match.arg(conf.type)
    conf.lower<- match.arg(conf.lower)
    if (is.logical(conf.int)) {
        # A common error is for users to use "conf.int = FALSE"
        #  it's illegal per documentation, but be kind
        if (!conf.int) conf.type <- "none"
        conf.int <- .95
    }

    type <- attr(Y, "type")
    if (type !='mright' && type!='mcounting' && 
        type != "right" && type != "counting")
        stop(paste("Cumulative incidence computation doesn't support \"", type,
                          "\" survival data", sep=''))

    n <- nrow(Y)
    status <- Y[,ncol(Y)]
    ncurve <- length(levels(X))
    
    state.names <- attr(Y, "states")
    if (missing(istate) || is.null(istate)) istate <- rep(0L, n)
    else if (is.factor(istate) || is.character(istate)) {
        # Match levels with the survival variable
        temp <- as.factor(istate)
        state.names <- unique(c(attr(Y, "states"), levels(istate)))
        istate <- as.numeric(factor(as.character(istate), levels=state.names))
    }
    else if (!is.numeric(istate) || any(istate != floor(istate)))
        stop("istate should be a vector of integers or a factor")
    
    if (length(id) ==0) id <- 1:n
    # these next two lines should be impossible, since istate came from the data frame
    if (length(istate) ==1) istate <- rep(istate,n)
    if (length(istate) !=n) stop ("wrong length for istate")

    states <- sort(unique(c(istate, 1:length(attr(Y, "states"))))) #list of all
    <<survfitCI-compute>>  
@
 
To make it easier to keep track of things in the computational kernel that does
all the real work, we reset the states, initial state, and status vectors
to all be integers 1, 2, \ldots, nstate, where ``1'' is the first state.
The status vector will have values of 0 for censored.
Per earlier discussion 1 will often be the unnamed initial state, which
will later be dropped from the output.
The statename vector is not modified.
<<survfitCI>>=
    if (any(states==0)) {
        state0 <- TRUE
        states <- states + 1
        istate <- istate + 1
        status <- ifelse(status==0, 0, status+1)
        }
    else state0 <- FALSE
    
    curves <- vector("list", ncurve)
    names(curves) <- levels(X)
                            
    if (ncol(Y)==2) {  # 1 transition per subject
        indx <- which(status == istate & status!=0)
        if (length(indx)) {
            warning("an observation transitions to it's starting state, ignored")
            status[indx] <- 0
        }
        if (length(id) && any(duplicated(id)))
            stop("Cannot have duplicate id values with (time, status) data")

        # dummy entry time that is < any event time
        entry <- rep(min(-1, 2*min(Y[,1])-1), n)  
        for (i in levels(X)) {
            indx <- which(X==i)
 #           temp  <- docurve1(entry[indx], Y[indx,1], status[indx], 
 #                                   istate[indx], weights[indx], states, 
 #                                   id[indx])
            curves[[i]] <- docurve2(entry[indx], Y[indx,1], status[indx], 
                                    istate[indx], weights[indx], states, 
                                    id[indx], se.fit)
         }
    }
    else {
        <<survfitCI-idcheck>>
        <<survfitCI-startstop>>
    }

    <<survfitCI-finish>>
}
@         
        
In the multi-state case we can calculate the current prevalence
vector $p(t)$ using the product-limit form
\begin{align*}
    p(t) &= p(0)\prod_{s<=t} [I + dA(s)] \\
         &= p(0) \prod_{s<=t} H(s)
\end{align*}
Where $p$ is a row vector and $H$ is the multi-state hazard matrix.  
At each event time we define 
$$H_{jk}(t) = \sum_i w_i dN_{ijk}(t)/ \sum w_iY_{ij}{t}$$
where $N_{ijk}$ counts the number of observed trasitions between 
state $j$ and state $k$ for subject $i$, $Y_{ij}(t)$ is 1 if subject $i$ is in
state $j$ at time $t$,  $w_i$ is the weight for subject $i$, 
and 0/0 is treated as 0.
Row $j$ of $H(t)$ describes the fate of those subjects in state $j$, going from
time $t$ to time $t+0$.  
The diagonal elements of $H$ are set so that each row of $H$ sums to 1
(everyone has to go somewhere). 
This formula collapses to the Kaplan-Meier in the simple case where $P(t)$ is a
vector of length 2 with state 1 = alive and state 2 = dead. 

A complementary formula is the hazard based calculation
$$S(t) = I-P = \exp^{-A(t)} $$
where exp is the matrix exponential and $A$ is the cumulative hazard matrix.
The matrix $A(t)$ has off diagonal elements $\sum_{s \le t} H(s)$, 
but the diagonal is chosen to give row sums of 0.
This collapses to $\exp(-\Lambda(t))$ in the alive/dead case.
The analog of the fh2 estimate comes from treating tied
event times of the same type (same states) as sequential.
The variance for these estimates is harder, so we haven't persued it in the
code yet.

A robust variance for the product-limit estimate is based on the
chain rule.  Consider the $n$ by $k$ matrix of per subject influence values
\begin{align}
  U_{ik}(t) &= \frac{\partial p_k(t)}{\partial w_i} \nonumber \\
            &= \frac{\partial[ p(t-) H_{.k}(t)]}{\partial w_i} \label{ci0} \\
           &= U{i.}(t-) H_{.k}(t) + 
                     p(t-) \frac{\partial H_{.k}(t)}{\partial w_i} \label{ci1}\\
 \frac{\partial H_{jk}(t)}{\partial w_i} &=  \left\{ \begin{array}{ll}
        \left( dN_{ijk}(t) - Y_{ij}(t)H_{jk} \right)/n_j(t) & j \ne k \\
        \left( -dN_{ij.}(t) - Y_{ij}(t)(H_{jj}-1) \right)/n_j(t) & j=k \\
       \end{array} \right. \label{ci2}
\end{align}
where $H_{.k}$ is the $k$th column of $H$ amd $n_j(t)= \sum_i Y_{ij}(t)w_i$
is the weighted number of subjects in state $j$.
Equation \eqref{ci0} replaces $p(t)$ with the last step of the compuation that
created it.  The next writes this out carefully using the chain rule, leading
to an recursive equation.
The first term of \eqref{ci1} is the formula for ordinary matrix multiplication.
In equation \eqref{ci2} the derivative of $H$ with respect to subject $i$
will be a matrix which is non-zero only for the row corresponding to the
current state of the subject.  (I've skipped some intermediate steps in
the derivation of \eqref{ci2}, ``left as an exercise for the reader'').

The weighted sum of each column of $U$ must zero (if computed correctly)
and the weighted sum of squares for each column will be the infinitesimal
jackknife estimate of variance for the elements of $p$.
The entire variance-covariance matrix for the states is $U'WU$ where $W$ is a diagonal 
matrix of case weights, but we currently don't report that back.

Below is the function for a single curve.
For the status variable a value if 0 is ``no event''.  
One nuisance in the function is that we need to ensure the
tapply command gives totals for all states, not just the ones present in the data
(a subgroup might not have them all) which lead to factor commands.
Another more confusing one is for multiple rows per subject data, where the 
cstate and U objects have only one row per subject.
This leads to indices of [[atrisk]] for the set of rows in the risk set but
[[aindx]] for the subjects in the risk set, [[death]] for the rows that have
an event as some given time and [[dindx]] for the corresponding subjects.
<<survfitCI-compute-old>>=
docurve1 <- function(entry, etime, status, istate, wt, states, id) {
    #
    # round off error can cause trouble: if two times are within machine
    #  precsion then "unique(time)" and the "table" command may differ
    # solve this by using creating a factor
    ftime <- factor(etime)
    ntime <- length(levels(ftime))
    timeset <- as.numeric(levels(ftime))
    ftime <- as.numeric(ftime)
    
    nstate <- length(states)
    Pmat <- matrix(0., nrow= ntime, ncol=nstate)
    vP <- Pmat  #variance
    A <- array(0., dim=c(nstate, nstate, ntime))
    uid <- sort(unique(id))
    U <- matrix(0., length(uid), nstate)  #one row per subject
    P <- as.vector(tapply(wt, factor(istate, levels=states), sum) / sum(wt))
    P <- Pmat[1,] <- ifelse(is.na(P), 0, P)
    cstate <- istate[match(uid, id)]   #current state for each observation
    
    nrisk <- integer(ntime)  #to be returned
    wrisk <- double(ntime)  #weighted number at risk
    nevent <- table(ftime, status>0)
    for (i in 1:ntime) {
        atrisk <- (ftime >=i & timeset[i] > entry)
        nrisk[i] <- sum(atrisk)
        wrisk[i] <- sum(wt[atrisk])
        tiedtime <- (ftime==i)
        if (nevent[i,2] ==0)  { # all censored here
            Pmat[i,] <- P
            if (i>1) {
                A[,,i] <- A[,,i-1]
                vP[i,] <- vP[i-1,]
                }
         }
        else {
            # do real work
            #  A bit of nuisance is to force tapply to give totals for all states
            aindx <- match(id[atrisk], uid)   #the id pointer for those at risk       
            ns <- as.vector(tapply(wt[atrisk], factor(cstate[aindx], levels=states),sum))
            dead <- which(tiedtime & status >0)  #the events at this time
            dindx <- match(id[dead], uid)
            nevent[i] <- length(dead)
 
            H <- tapply(wt[dead], list(factor(cstate[dindx], levels=states),
                                       factor(status[dead], levels=states)),sum)/ns
            H <- ifelse(is.na(H), 0, H) # H has NA for combinations with no representatives
            diag(H) <- 1- rowSums(H)
          
            H2 <- H
            diag(H2) <- diag(H2) -1  #version of H needed for U and A, rows sum to 0
            if (i==1) A[,,1] <- H2
            else      A[,,i] <- A[,,i-1] + H2
            newstate <- status[dead]    # where the transitions go, will never be 0
            oldstate <- cstate[dindx]   # where they came from
            U <- U%*%H   #first part of update
            
            U[aindx,] <- U[aindx,] - (P*H2/ns)[cstate[aindx], ]
            temp <- P[oldstate]/ns[oldstate]    #the extra update for the events
            U[cbind(dindx, oldstate)] <- U[cbind(dindx, oldstate)] - temp
            U[cbind(dindx, newstate)] <- U[cbind(dindx, newstate)] + temp
            cstate[dindx] <- newstate
            P <- Pmat[i,] <- c(P %*% H)
            vP[i,] <- colSums(wt[match(uid, id)]*U*U)
        }
    }
    list(time =as.vector(timeset), pmat=Pmat, std=sqrt(vP),
         n.event= as.vector(nevent[,2]), n.risk= as.vector(nrisk),
         w.risk=wrisk, cumhaz=A)
}
@ 

The above function was used to work through all of my test cases, 
but is too slow in large data sets.
Rewrite it using underlying C-code, but retain the former one for
debugging purposes.  It appears at the end of this chapter.

The setup for (start, stop] data is a bit more work.  
We want to ensure that a subject's weight is fixed, that they have a
continuous period of observation, and that they don't
transfer from a state to itself.

<<survfitCI-idcheck>>=
if (missing(id) || is.null(id))
    stop("the id argument is required for start:stop data")

indx <- order(id, Y[,2])  #ordered event times within subject
indx1 <- c(NA, indx)  #a pair of lagged indices
indx2 <- c(indx, NA)
same <- (id[indx1] == id[indx2] & !is.na(indx1) & !is.na(indx2)) #indx1, indx2= same id?
if (any(same & X[indx1] != X[indx2])) {
    who <- 1 + min(which(same & X[indx1] != X[indx2]))
    stop("subject is in two different groups, id ", (id[indx1])[who])
}
if (any(same & Y[indx1,2] != Y[indx2,1])) {
    who <- 1 + min(which(same & Y[indx1,2] != Y[indx2,1]))
    stop("gap in follow-up, id ", (id[indx1])[who])
}
if (any(Y[,1] == Y[,2])) 
    stop("cannot have start time == stop time")

if (any(same & Y[indx1,3] == Y[indx2,3] & Y[indx1,3] !=0)) {
    who <-  1 + min(which(same & Y[indx1,1] != Y[indx2,2]))
    stop("subject changes to the same state, id ", (id[indx1])[who])
}
if (any(same & weights[indx1] != weights[indx2])) {
    who <-  1 + min(which(same & weights[indx1] != weights[indx2]))
    stop("subject changes case weights, id ", (id[indx1])[who])
}
@ 

<<survfitCI-startstop>>=
# We only want to pay attention to the istate variable for the very first
#  observation of any given subject, but the program logic does better with
#  a full one.  So construct one that will do this
indx <- order(Y[,2])
uid <- unique(id)
temp <- (istate[indx])[match(uid, id[indx])]  #first istate for each subject
istate <- temp[match(id, uid)]  #replicate it to full length

# Now to work
for (i in levels(X)) {
    indx <- which(X==i)
#    temp <- docurve1(Y[indx,1], Y[indx,2], status[indx], 
#                          istate[indx], weights[indx], states, id[indx])
    curves[[i]] <- docurve2(Y[indx,1], Y[indx,2], status[indx], 
                          istate[indx], weights[indx], states, id[indx], se.fit)
}
@ 
            
<<survfitCI-finish>>= 
# Turn the result into a survfit type object
grabit <- function(clist, element) {
    temp <-(clist[[1]][[element]]) 
    if (is.matrix(temp)) {
        nc <- ncol(temp)
        matrix(unlist(lapply(clist, function(x) t(x[[element]]))),
                        byrow=T, ncol=nc)
        }
    else as.vector(unlist(lapply(clist, function(x) x[element])))
    }
kfit <- list(n =      as.vector(table(X)),
             time =   grabit(curves, "time"),
             n.risk=  grabit(curves, "n.risk"),
             n.event= grabit(curves, "n.event"),
             n.censor=grabit(curves, "n.censor"),
             prev   = grabit(curves, "pmat"))
nstate <- length(states)
kfit$cumhaz <- array(unlist(lapply(curves, function(x) x$cumhaz)),
                           dim=c(nstate, nstate, length(kfit$time)))
if (length(curves) >1)
    kfit$strata <- unlist(lapply(curves, function(x) length(x$time)))
if (se.fit) kfit$std.err <- grabit(curves, "std")

# if state 0 was present, remove it
if (state0) {
    kfit$prev <- kfit$prev[,-1]
    if (se.fit) kfit$std.err <- kfit$std.err[,-1]
}
@ 

Add the confidence bands.  The idea is modeled on survfitKM but with
the important differences that we are dealing with $P$ instead of $S$,
and the ``modified lower limit'' logic does not apply.
We make the assumption that $\log(1-P)$ will have better CI behavior
than $P$, with standard error of ${rm se}(P)/(1-P)$.

<<survfitCI-finish>>=
#       
# Last bit: add in the confidence bands:
#   modeled on survfit.km, though for P instead of S
#   
#
if (se.fit) {
    std.err <- kfit$std.err
    zval <- qnorm(1- (1-conf.int)/2, 0,1)
    surv <- 1-kfit$prev

    if (conf.type=='plain') {
        temp <- zval* std.err
        kfit <- c(kfit, list(lower =pmax(kfit$prev-temp, 0), 
                             upper=pmin(kfit$prev+temp, 1),
                         conf.type='plain', conf.int=conf.int))
        }

    if (conf.type=='log') {
        #avoid some "log(0)" messages
        xx <- ifelse(kfit$prev==1, 1, 1- kfit$prev)  

        temp1 <- ifelse(surv==0, NA, exp(log(xx) + zval* std.err/xx))
        temp2 <- ifelse(surv==0, NA, exp(log(xx) - zval* std.err/xx))
        kfit <- c(kfit, list(lower=pmax(1-temp1,0), upper= 1- temp2,
                         conf.type='log', conf.int=conf.int))
        }

    if (conf.type=='log-log') {
        who <- (surv==0 | surv==1) #special cases
        temp3 <- ifelse(surv==0, NA, 1)
        xx <- ifelse(who, .1,kfit$surv)  #avoid some "log(0)" messages
        temp1 <- exp(-exp(log(-log(xx)) + zval*std.err/(xx*log(xx))))
        temp1 <- ifelse(who, temp3, temp1)
        temp2 <- exp(-exp(log(-log(xx)) - zval*std.err/(xx*log(xx))))
        temp2 <- ifelse(who, temp3, temp2)
        kfit <- c(kfit, list(lower=1-temp1, upper=1-temp2,
                         conf.type='log-log', conf.int=conf.int))
        }
    }

kfit$states <- state.names
kfit$type   <- attr(Y, "type")
kfit
@

The updated docurve function is here
<<survfitCI-compute>>=
docurve2 <- function(entry, etime, status, istate, wt, states, id, se.fit) {
    #
    # round off error can cause trouble, if two times are within machine
    #  precsion
    # solve this by using creating a factor
    ftime <- factor(c(entry,etime))
    ltime <- levels(ftime)
    ftime <- matrix(as.integer(ftime), ncol=2)
    n <- length(entry)
    timeset <- as.numeric(ltime[sort(unique(ftime[,2]))]) #unique event times
     
    nstate <- length(states)
    uid <- sort(unique(id))
    P <- as.vector(tapply(wt, factor(istate, levels=states), sum) / sum(wt))
    P <- ifelse(is.na(P), 0, P) # initial probability distribution
    cstate <- istate[match(uid, id)]   #current state for each observation
    
    storage.mode(wt) <- "double" # just in case someone had integer weights
    storage.mode(cstate) <- "integer"
    storage.mode(status) <- "integer"
    # C code has 0 based subscripts
    fit <- .Call(Csurvfitci, ftime, 
                 order(ftime[,1]) - 1L,
                 order(ftime[,2]) - 1L,
                 length(timeset),
                 status,
                 cstate - 1L,
                 wt,
                 match(id, uid) -1L,
                 P, as.integer(se.fit))
    if (se.fit) 
        list(time=timeset, pmat=t(fit$p), std=sqrt(t(fit$var)),
             n.risk = colSums(fit$nrisk),n.event = fit$nevent, 
             n.censor=fit$ncensor, 
             cumhaz=array(fit$cumhaz, dim=c(nstate,nstate, length(timeset))))
    else list(time=timeset, pmat=t(fit$p),
             n.risk = colSums(fit$nrisk),n.event = fit$nevent, 
             n.censor=fit$ncensor, 
             cumhaz=array(fit$cumhaz, dim=c(nstate,nstate, length(timeset))))
}
@
