\section{The Fine-Gray model}
For competing risks with ending states 1, 2, \ldots $k$, 
the Fine-Gray approach turns these into a set of simple 2-state
Cox models:
\begin{itemize}
  \item (not yet in state 1) $\longrightarrow$ state 1
  \item (not yet in state 2) $\longrightarrow$ state 2
  \item \ldots
\end{itemize}
Each of these is now a simple Cox model, assuming that we are willing
to make a proportional hazards assumption.
There is one added complication:
when estimating the first model, one wants to use the data set that
would have occured if the subjects being followed for state 1 had
not had an artificial censoring, that is, had been clinically followed
as though they were still in the denominator of subjects who are at
risk of transition to state 1.
Sometimes this can be filled in directly, e.g., if we knew the enrollment
dates for each subject along with the date that follow-up for the
study was terminated, and there was no lost to follow-up (only administrative
censoring.)
In practice what is done is to estimate the overall censoring distribution and
give subjects artificial follow-up.

The function below creates a data set that can then be used with coxph.
It has a lot of commonality with \code{survSplit}.

<<finegray>>= 
finegray <- function(formula, data, subset, na.action,
                     endpoint, prefix="fg", count="", id) {
    Call <- match.call()
    indx <- match(c("formula", "data", "subset", "na.action"),
              names(Call), nomatch=0) 
    if (indx[1] ==0) stop("A formula argument is required")
    temp <- Call[c(1,indx)]  # only keep the arguments we wanted
    temp[[1L]] <- quote(stats::model.frame)  # change the function called

    special <- c("strata", "cluster")
    temp$formula <- if(missing(data)) terms(formula, special)
    else              terms(formula, special, data=data)

    mf <- eval(temp, parent.frame())
    if (nrow(mf) ==0) stop("No (non-missing) observations")
    Terms <- terms(mf)

    Y <- model.extract(mf, "response")
    if (!inherits(Y, "Surv")) stop("Response must be a survival object")
    type <- attr(Y, "type")
    if (type!='mright' && type!='mcounting')
	stop("Fine-Gray model requires a multi-state survival")
    nY <- ncol(Y)
    states <- attr(Y, "states")

    strats <- attr(Terms, "specials")$strata
    if (length(strats)) {
	stemp <- untangle.specials(Terms, 'strata', 1)
	if (length(stemp$vars)==1) strata <- mf[[stemp$vars]]
	else strata <- survival::strata(mf[,stemp$vars], shortlabel=TRUE)
        istrat <- as.numeric(strata)
        mf[stemp$vars] <- NULL
	}
    else istrat <- rep(1, nrow(mf))
    
    id <- model.extract(mf, "id")
    cluster<- attr(Terms, "specials")$cluster
    if (length(cluster)) {
        if (!is.null(id)) stop("an id argument and a cluster() term are redundant")
        tempc <- untangle.specials(Terms, 'cluster', 1)
        id <- strata(mf[,tempc$vars], shortlabel=TRUE)  #allow multiples
        mf[tempc$vars] <- NULL
    }
    
    # If there is start-stop data, then there needs to be a cluster()
    #  argument or an id argument, and we check that this is indeed
    #  a competing risks form of data.
    if (type=="mcounting") {
        if (is.null(id)) stop("(start, stop] data requires a subject id")
        else {
            index <- order(id, Y[,2]) # by time within id
            temp <- Y[index,]
            first <- which(!duplicated(id[index]))
            last  <- c(first[-1] -1, length(id))
            if (any(temp[-last, 3]) != 0)
                stop("a subject has a transition before their last time point")
            delta <- c(temp[-1,1], 0) - temp[,2]
            if (any(delta[-last] !=0)) 
                stop("a subject has gaps in time")
            if (any(Y[first,1] > min(Y[,2])))
                stop("the routine does not allow for delayed entry")
        }
    }

    if (missing(endpoint)) enum <- 1  #generate a data set for which endpoint?
    else {
        index <- match(endpoint, states)
        if (any(is.na(index)))
            stop ("endpoint argument has a state that is not in the data")
        enum <- index[1]
        if (length(index) > 1) warning("only the first endpoint was used")
    }
    
    # make sure count, if present is syntactically valid
    if (!missing(count)) count <- make.names(count) else count <- NULL
    oname <- paste0(prefix, c("time1", "time2", "status", "wt"))
        
    <<finegray-censor>>   
    <<finegray-build>>
}  
@

The censoring distribution is estimated as a simple KM with one modification.
In a time to death KM say that subject A has an event and subject B is censored
on the exact same day.  We assume that B was at risk for said event when 
computing the estimate.  The set of subjects at risk is left continuous
and the estimated survival is right-continuous.
For the reverse case of computing the censoring distribution, the FG estimate
does not assume that subject A was at risk of censoring on that day.
The risk set is right-continuous and the function left continuous.
A fix to this is break any ties by making
all the event times just a little bit shorter.
To avoid issues with times that are nearly identical (but not quite) we first
convert to an integer time scale.
Since this is a competing risks data set any non-censored observation for a
subject is their last, so this time shift does not goof up the alignment
of start, stop data.

<<finegray-censor>>=
find2 <- function(x, vec, left.open=FALSE, ...) {
    if (!left.open) findInterval(x, vec, ...)
    else {
        # the left.open arg is a recent addition to findInterval, and I want
        #  this to work in 3.2.0 (institutional installs).  In another cycle or
        #  so we can drop this workaround
        #
        length(vec) - findInterval(-x, rev(-vec), ...)
    }
}
    
if (ncol(Y) ==2) {
    temp <- min(Y[,1], na.rm=TRUE)
    if (temp >0) zero <- 0
    else zero <- 2*temp -1  # a value less than any observed y
    Y <- cbind(zero, Y)  # add a start column
}
utime <- sort(unique(Y[,2]))  # all the unique end times
newtime <- matrix(findInterval(Y[,1:2], utime), ncol=2) 
status <- Y[,3]
newtime[status !=0, 2] <- newtime[status !=0,2] - .2
csurv <- survfit(Surv(newtime[,1], newtime[,2], status==0) ~ istrat, se.fit=FALSE)
@ 

Consider the following data set: 
\begin{itemize}
  \item Events of type 1 at times 1, 4, 5,  10
  \item Events of type 2 at times 2, 5, 8
  \item Censors at times 3, 4, 4, 6, 8, 9, 12
\end{itemize}
The censoring distribution will have the following shape:
\begin{center}
  \begin{tabular}{rcccccc}
    interval& (0,3]& (3,4] & (4,6]         & (6,8]       & (8,12] & 12+\\
    C(t)    &  1   &11/12  & (11/12)(8/10) & (11/15)(5/6)&  (11/15)(5/6)(3/4)&
       0 \\
       & 1.0000 & .9167 & .7333 & .6111 & .4583
    \end{tabular}
  \end{center}
Notice that at time 4, the event at time 4 is not counted in the risk set
so the jump is 8/10 rather than 8/11. 
Likewise at time 8 the risk set has 4 instead of 5:
censors occur after deaths.

When creating the data set for event type 1, subjects who have an event of
type 2 get extended out using this censoring distribution.  The event at
time 2, for instance, appears as a censored observation with time dependent
weights of $C(t)$.  The type 2 event at time 5 has weight 1 up through time 5,
then weights of $C(t)/C(5)$ for the remainder.
This means a weight of 1 over (5,6], 5/6 over (6,8], (5/6)(3/4) over (9,12]
and etc. 

Though there are 6 unique censoring intervals, 
in the created data set for event type 1 we only need to know case
weights at times 1, 4, 5, and 10; the information from the (4,6] and (6,8] 
intervals will never be used.  
To create a minimal sized data set we can leave those intervals out. 
$C(t)$ only drops to zero if the largest time(s) are censored observations, so
by definition no events lie in an interval with $C(t)=0$.

<<finegray-build>>=
status <- Y[, 3]

# keep only the event times, and convert back to the original time units
ctime <- utime[csurv$time[csurv$n.event > 0]]
cprob <- csurv$surv[csurv$n.event > 0]
if (is.null(csurv$strata)) cstrat <- rep(1, length(ctime))  
else {
    cstrat <- (rep(1:length(csurv$strata), csurv$strata))[csurv$n.event > 0]
}
maxtime <- max(Y[,2])

# Do computations separately for each stratum
stratfun <- function(i) {
    keep <- (istrat ==i)
    tdata <- mf[keep, -1, drop=FALSE]
    times <- sort(unique(Y[keep & status == enum, 2])) #unique event times 
    if (length(times)==0) return(NULL)  #no events in this stratum

    ct2 <- c(ctime[cstrat==i], maxtime)
    cp2 <- c(1.0, cprob[cstrat==i])
    index <- find2(times, ct2, left.open=TRUE)
    index <- sort(unique(index))  # the intervals that were actually seen
    # times before the first ctime get index 0, those between 1 and 2 get 1
    ckeep <- rep(FALSE, length(ct2))
    ckeep[index] <- TRUE
    expand <- (Y[keep, 3] !=0 & Y[keep,3] != enum) #which rows to expand
    split <- .Call("finegray", Y[keep,1], Y[keep,2], ct2, cp2, expand, 
                   c(TRUE, ckeep)) 
    tdata <- tdata[split$row,,drop=FALSE]
    tstat <- ifelse(status[split$row]== enum, 1, 0)

    tdata[[oname[1]]] <- split$start
    tdata[[oname[2]]] <- split$end
    tdata[[oname[3]]] <- tstat
    tdata[[oname[4]]] <- split$wt
    if (!is.null(count)) tdata[[count]] <- split$add
    tdata
}

if (max(istrat) ==1) result <- stratfun(1)
else {
    tlist <- lapply(1:max(istrat), stratfun)
    result <- do.call("rbind", tlist)
}

rownames(result) <- NULL   #remove all the odd labels that R adds
attr(result, "event") <- states[enum]
result
@ 
