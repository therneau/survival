
\section{Residuals for survival curves}
\subsection{R-code}
For all the more complex cases, the variance of a survival curve is based on 
the infinitesimal jackknife:
$$
D_i(t) = \frac{\partial S(t)}{\partial w_i}
$$
evaluated at the the observed vector of weights.  The variance at a given 
time is then  $D'WD'$ where $D$ is a diagonal matrix of the case weights.
When there are multiple states $S$ is replaced by the vector $p(t)$, with
one element per state, and the formula gets a bit more complex.
The predicted curve from a Cox model is the most complex case.

Realizing that we need to return the matrix $D$ to the user, in order to compute
the variance of derived quantities like the restricted mean time in state, 
the code has been changed from a primarily internal focus (compute within the
survfit routine) to an external one. 

The underlying C code is very similar to that in survfitkm.c
One major difference in the routines is that this code is designed to return
values at a fixed set of time points; it is an error if the user does not
provide them.  This allows the result to be presented as a matrix or array.
Computational differences will be discussed later.

The method argument is for debugging.  For multi-state it uses either C code
or the optimized R method.
The double call below is because we want residuals to return a simple matrix,
but the pseudo function needs to get back a little bit more.

<<residuals.survfit>>=
# residuals for a survfit object
residuals.survfit <- function(object, times, 
                              type= "pstate",
                              collapse, weighted=FALSE, method=1, ...){

    if (!inherits(object, "survfit"))
        stop("argument must be a survfit object")
    if (object$type=="interval") {
        # trial code to support it
        # reconstruct the data set
        # create dummy time/status for all interval or left censored
        #   over the span of jump points in S, non-censored obs with
        #   weights proportional to the jumps
        # combine dummy + (exact, right) from original, compute KM
        # get pseudo for this new KM
        # collapse dummy obs back to a single
        stop("residuals for interval-censored data are not available")
        }
    if (!is.null(object$oldstates)) 
        stop("residuals not available for a subscripted survfit object")
    if (missing(times)) stop("the times argument is required")
    # allow a set of alias
    temp <- c("pstate", "cumhaz", "sojourn", "survival",
                              "chaz", "rmst", "rmts", "auc")
    type <- match.arg(casefold(type), temp)
    itemp <-  c(1,2,3,1,2,3,3,3)[match(type, temp)]
    type <- c("pstate", "cumhaz", "auc")[itemp]

    if (missing(collapse)) 
         fit <- survresid.fit(object, times, type, weighted=weighted, 
                              method= method)
    else fit <- survresid.fit(object, times, type, collapse= collapse, 
                              weighted= weighted, method= method)

    fit$residuals
}

survresid.fit <- function(object, times, 
                              type= "pstate",
                              collapse, weighted=FALSE, method=1) {
    if (object$type=="interval") stop("interval censored not yet supported")
    survfitms <- inherits(object, "survfitms")
    coxsurv <- inherits(object, "survfitcox")  # should never be true, as there
                                               #  is a residuals.survfitcox
    timefix <- (is.null(object$timefix) || object$timefix)
    
    start.time <- object$start.time
    if (is.null(start.time)) start.time <- min(c(0, object$time))

    # check input arguments
    if (missing(times)) 
        stop("the times argument is required")
    else {
        if (!is.numeric(times)) stop("times must be a numeric vector")
        times <- sort(unique(times))
        if (timefix) times <- aeqSurv(Surv(times))[,1]
    }

    # get the data
    <<rsurvfit-data>>

    if (missing(collapse)) collapse <- (!(is.null(id)) && any(duplicated(id)))
    if (collapse && is.null(id)) stop("collapse argument requires an id or cluster argument in the survfit call")

    ny <- ncol(newY)
    if (collapse && any(X != X[1])) {
        # If the same id shows up in multiple curves, we just can't deal
        #  with it.
        temp <- unlist(lapply(split(id, X), unique))
        if (any(duplicated(temp)))
            stop("same id appears in multiple curves, cannot collapse")
    }
    
    timelab <- signif(times, 3)  # used for dimnames
    # What type of survival curve?
    stype <- Call$stype
    if (is.null(stype)) stype <- 1
    ctype <- Call$ctype
    if (is.null(ctype)) ctype <- 1
    if (!survfitms) {
        resid <- rsurvpart1(newY, X, casewt, times,
                            type, stype, ctype, object)
        if (collapse) {
            resid <- rowsum(resid, id, reorder=FALSE)
            dimnames(resid) <- list(id= unique(id), times=timelab)
            curve <- (as.integer(X))[!duplicated(id)] #which curve for each
        } 
        else {
            if (length(id) >0) dimnames(resid) <- list(id=id, times=timelab)
            curve <- as.integer(X)
        }
    }
    else {  # multi-state
        if (!collapse) {
            if (length(id >0)) d1name <- id else d1name <- NULL
            cluster <- d1name
            curve <- as.integer(X)
        }       
        else {
            d1name <- unique(id)
            cluster <- match(id, d1name)
            curve <- (as.integer(X))[!duplicated(id)]
        }
        resid <- rsurvpart2(newY, X, casewt, istate, times, cluster,
                            type, object, method=method, collapse=collapse)

        if (type == "cumhaz") {
            ntemp <- colnames(object$cumhaz)
            if (length(dim(resid)) ==3)
                 dimnames(resid) <- list(id=d1name, times=timelab, 
                                         cumhaz= ntemp)
            else dimnames(resid) <- list(id=d1name, cumhaz=ntemp)
        }
        else {
            ntemp <- object$states
            if (length(dim(resid)) ==3) 
                dimnames(resid) <- list(id=d1name, times=timelab, 
                                        state= ntemp)
            else dimnames(resid) <- list(id=d1name, state= ntemp)
        }
    }

    if (weighted && any(casewt !=1)) resid <- resid*casewt

    list(residuals= resid, curve= curve, id= id, idname=idname)
}
@ 

The first part of the work is retrieve the data set.  This is done in multiple
places in the survival code, all essentially the same.  
If I gave up (like lm) and forced the model frame to be saved this would be
easier of course.

<<rsurvfit-data>>=
Call <- object$call
Terms <- object$terms

# remember the name of the id variable, if present.
#  but we don't try to parse it:  id= mydata$clinic becomes NULL
idname <- Call$id
if (is.name(idname)) idname <- as.character(idname)
else idname <- NULL   
# I always need the model frame
mf <- model.frame(object)
if (is.null(object$y)) Y <- model.response(mf)
else Y <- object$y

formula <- formula(object)
# the chunk below is shared with survfit.formula 
na.action <- getOption("na.action")
if (is.character(na.action))
    na.action <- get(na.action)  # a hack to allow the shared code
<<survfit.formula-getdata>>
# end of shared code 

xlev <- levels(X)

# Deal with ties
if (is.null(Call$timefix) || Call$timefix) newY <- aeqSurv(Y) else newY <- Y
@

This code has 3 primary sections: single state survival, multi-state survival,
and post-Cox survival.  
A motivating idea in all of them is to avoid an $O(nd)$ calculation that 
involves the increment to each subject's leverage at each of the $d$
event times.  Since $d$ often grows with $n$ this can get very slow.  This
routine is designed for the case where the number of time points in the 
output matrix is modest, so we aim for $O(n)$ processes that repeat for
each output time.

\subsection{Simple survival}
The Nelson-Aalen estimate of cumulative hazard is a simple sum
\begin{align}
  H(t) &= H(t-) + h(t) \nonumber \\
  \frac{\partial H(t)}{\partial w_i} &= \frac{\partial H(t-)}{\partial w_i} +
       [dN_i(t) - Y_i(t)h(t)]/r(t) \nonumber \\
       &= \sum_{d_j \le t} dN_i(d_j)/r(d_j) - Y_i(d_j)h(d_j)/r(d_j) 
         \label{NAderiv}
\end{align}
where $H$ the cumulative hazard, 
$h$ is the increment to the cumulative hazard, $Y_i$ is 1 when a
subject is at risk, and $dN_i$ marks an event for the subject.
Our basic strategy for the NA estimate is to use a two stage estimate.
First, compute three vectors, each with one element per event time.
\begin{itemize}
  \item term1 = $1/r(d_j)$ is the increment to the derivative for any
    observation with an event at event time $d_j$
  \item term2 = $-h(d_j)/r(d_j)$ is the increment for any observation that is at
    risk at time $d_j$
  \item term3 = cumulative sum of term2
\end{itemize}

For any given observation $i$ whose follow-up interval is $(s_i, t_i)$, their
derivative at time $z$ is the sum of
\begin{itemize}
  \item term3(min($z$, $t_i$)) - term3(min($z$, $s_i$))
  \item term1($t_i$) if $t_i \le z$ and observation $i$ is an event
\end{itemize}
The computation of term1 and term3 are each $O(d)$, the number of events, and
the residual is $O(2n)$, an addition is done when it enters the risk set and
another when it leaves.  This accomplishes our goal to not update every member
of the risk set at every event.


The Fleming-Harrington estimate of survival is 
\begin{align*}
  S(t) &= e^{-H(t)} \\
  \frac{\partial S(t)}{\partial w_i} &=  -S(t)\frac{\partial H(t)}{\partial w_i} 
\end{align*}
So has exactly the same computation, with a multiplication at the end.

<<residuals.survfit>>=
rsurvpart1 <- function(Y, X, casewt, times,
         type, stype, ctype, fit) {
     
    ntime <- length(times)
    etime <- (fit$n.event >0)
    ny <- ncol(Y)
    event <- (Y[,ny] >0)
    status <- Y[,ny]

    # 
    #  Create a list whose first element contains the location of
    #   the death times in curve 1, second element the death times for curve 2,
    #  
    if (is.null(fit$strata)) {
        fitrow <- list(which(etime))
    }
    else {
        temp1 <- cumsum(fit$strata)
        temp2 <- c(1, temp1+1)
        fitrow <- lapply(1:length(fit$strata), function(i) {
            indx <- seq(temp2[i], temp1[i])
            indx[etime[indx]] # keep the death times
        })
    }
    ff <- unlist(fitrow) 
 
    # for each time x, the index of the last death time which is <=x.
    #  0 if x is before the first death time in the fit object.
    #  The result is an index to the survival curve
    matchfun <- function(x, fit, index) {
        dtime <- fit$time[index]  # subset to this curve
        i2 <- findInterval(x, dtime, left.open=FALSE)
        c(0, index)[i2 +1]
    }
     
    # output matrix D will have one row per observation, one col for each
    #  reporting time. tindex and yindex have the same dimension as D.
    # tindex points to the last death time in fit which
    #  is <= the reporting time.  (If there is only 1 curve, each col of
    #  tindex will be a repeat of the same value.)
    tindex <- matrix(0L, nrow(Y), length(times))
    for (i in 1:length(fitrow)) {
        yrow <- which(as.integer(X) ==i)
        temp <- matchfun(times, fit, fitrow[[i]])
        tindex[yrow, ] <- rep(temp, each= length(yrow))
    }
    tindex[,] <- match(tindex, c(0,ff)) -1L  # the [,] preserves dimensions

    # repeat the indexing for Y onto fit$time.  Each row of yindex points
    #  to the last row of fit with death time <= Y[,ny]
    ny <- ncol(Y)
    yindex <- matrix(0L, nrow(Y), length(times))
    event <- (Y[,ny] >0)
    if (ny==3) startindex <- yindex
    for (i in 1:length(fitrow)) {
        yrow <- (as.integer(X) ==i)  # rows of Y for this curve
        temp <- matchfun(Y[yrow,ny-1], fit, fitrow[[i]])
        yindex[yrow,] <- rep(temp, ncol(yindex))
        if (ny==3) {
            temp <- matchfun(Y[yrow,1], fit, fitrow[[i]])
            startindex[yrow,] <- rep(temp, ncol(yindex))
        }
    }                    
    yindex[,] <- match(yindex, c(0,ff)) -1L
    if (ny==3) {
        startindex[,] <- match(startindex, c(0,ff)) -1L
        # no subtractions for report times before subject's entry
        startindex <- pmin(startindex, tindex) 
    }
    
    # Now do the work
    if (type=="cumhaz" || stype==2) {  # result based on hazards
        if (ctype==1) {
            <<residpart1-nelson>>
        } else {
            <<residpart1-fleming>>
        }
    } else { # not hazard based
        <<residpart1-AJ>>
    }
    D
}
@

The Nelson-Aalen is the simplest case. 
We don't have to worry about case weights of the data, since that has
already been accounted for by the survfit function.

<<residpart1-nelson>>=
death <- (yindex <= tindex & rep(event, ntime)) # an event occured at <= t

term1 <- 1/fit$n.risk[ff]
term2 <- lapply(fitrow, function(i) fit$n.event[i]/fit$n.risk[i]^2)
term3 <- unlist(lapply(term2, cumsum))

sum1 <- c(0, term1)[ifelse(death, 1+yindex, 1)]
sum2 <- c(0, term3)[1 + pmin(yindex, tindex)]
if (ny==3) sum3 <- c(0, term3)[1 + pmin(startindex, tindex)]

if (ny==2) D <- matrix(sum1 -  sum2, ncol=ntime)
else       D <- matrix(sum1 + sum3 - sum2, ncol=ntime)

# survival is exp(-H) so the derivative is a simple transform of D
if (type== "pstate") D <- -D* c(1,fit$surv[ff])[1+ tindex]
else if (type == "auc") {
    <<auctrick>>
}
@

The sojourn time is the area under the survival curve. Let $x_j$ be the
widths of the rectangles under the curve from event time $d_j$ to
$\min(d_{j+1}, t)$, zero if $t \le d_j$, or $t-d_m$ if $t$ is after the last
event time.
\begin{align}
  A(0,t) &= \sum_{j=1}^m x_j S(d_j) \\nonumber \\
  \frac{\partial A(0,t)}{\partial w_i} &=
   \sum_{j=1}^m -x_j S(d_j) \frac{\partial H(d_j)}{\partial w_i} \nonumber \\
  &= \sum_{j=1}^m -x_jS(d_j) \sum_{k \le j} \frac{\partial h(d_k)}{\partial w_i} 
   \nonumber \\
  &= \sum_{k=1}^m \frac{\partial h(d_k)}{\partial w_i} 
          \left(\sum_{j\ge k} -x_j S(d_j) \right) \nonumber \\
  &= \sum_{k=1}^m -A(d_k, t) \frac{\partial h(d_k)}{\partial w_i} 
          \label{eq:auctrick}   
\end{align}

For an observation at risk over the interval $(a,b)$ we have exactly the same
calculus as the cumulative hazard with respect to which $h(d_k)$ terms
are counted for the observation, but now they are weighted sums.  The weights
are different for each output time, so we set them up as a matrix.
We need the AUC at each event time $d_k$, and the AUC at the output times.

Matrix subscripts are a little used feature of R. If y is a matrix of
values and x is a 2 colum matrix containing m (row, col) pairs, the
result will be a vector of length m that plucks out the [x[1,1], x[1,2]]
value of y, then the [x[2,1], x[2,2]] value of y, etc.
They are rarely useful, but very handy in the few cases where they apply.

<<auctrick>>=
auc1 <- lapply(fitrow, function(i) {
             if (length(i) <=1) 0
             else c(0, cumsum(diff(fit$time[i]) * (fit$surv[i])[-length(i)]))
                 })  # AUC at each event time
auc2 <- lapply(fitrow, function(i) {
             if (length(i) <=1) 0
             else {
                 xx <- sort(unique(c(fit$time[i], times))) # all the times
                 yy <- (fit$surv[i])[findInterval(xx, fit$time[i])]
                 auc <- cumsum(c(diff(xx),0) * yy)
                 c(0, auc)[match(times, xx)]
                 }})  # AUC at the output times

# Most often this function is called with a single curve, so make that case
#  faster.  (Or I presume so: mapply and do.call may be more efficient than 
#  I think for lists of length 1).
if (length(fitrow)==1) { # simple case, most common to ask for auc 
    wtmat <- pmin(outer(auc1[[1]], -auc2[[1]], '+'),0)
    term1 <- term1 * wtmat
    term2 <- unlist(term2) * wtmat
    term3 <- apply(term2, 2, cumsum)
}
else { #more than one curve, compute weighted cumsum per curve
    wtmat <- mapply(function(x, y) pmin(outer(x, -y, "+"), 0), auc1, auc2)
    term1 <- term1 * do.call(rbind, wtmat)
    temp <- mapply(function(x, y) apply(x*y, 2, cumsum), term2, wtmat)
    term3 <- do.call(rbind, temp)
}

sum1 <- sum2 <- matrix(0, nrow(yindex), ntime)
if (ny ==3) sum3 <- sum1
for (i in 1:ntime) {
    sum1[,i] <- c(0, term1[,i])[ifelse(death[,i], 1 + yindex[,i], 1)]
    sum2[,i] <- c(0, term3[,i])[1 + pmin(yindex[,i], tindex[,i])]
    if (ny==3) sum3[,i] <- c(0, term3[,i])[1 + pmin(startindex[,i], tindex[,i])]
}
# Perhaps a bit faster(?), but harder to read. And for AUC people usually only
#  ask for one time point
#sum1 <- rbind(0, term1)[cbind(c(ifelse(death, 1+yindex, 1)), c(col(yindex)))]
#sum2 <- rbind(0, term3)[cbind(c(1 + pmin(yindex, tindex)), c(col(yindex)))]
#if (ny==3) sum3 <- 
#             rbind(0, term3)[c(cbind(1 + pmin(startindex, tindex)), 
#                               c(col(yindex)))]
if (ny==2) D <- matrix(sum1 -  sum2, ncol=ntime)
else       D <- matrix(sum1 + sum3 - sum2, ncol=ntime)
@

\paragraph{Fleming-Harrington}
For the Fleming-Harrington estimator the calculation at a tied time differs
slightly.
If there were 10 at risk and 3 tied events, the Nelson-Aalen has an increment
of 3/10, while the FH has an increment of (1/10 + 1/9 + 1/8).  The underlying
idea is that the true time values are continuous and we observe ties due to
coarsening of the data.  The derivative will have 3 terms as well.  In this
case the needed value cannot be pulled directly from the survfit object.
Computationally, the number of distinct times at which a tie occurs is normally
quite small and the for loop below will not be too expensive.

<<residpart1-fleming>>=
stop("residuals function still imcomplete, for FH estimate")
if (any(casewt != casewt[1])) {
    # Have to reconstruct the number of obs with an event, the curve only
    # contains the weighted sum
    nevent <- unlist(lapply(seq(along.with=levels(X)), function(i) {
        keep <- which(as.numeric(X) ==i)
        counts <- table(Y[keep, ny-1], status)
        as.vector(counts[, ncol(counts)])
        }))
} else nevent <- fit$n.event

n2 <- fit$n.risk
risk2 <- 1/fit$n.risk
ltemp <- risk2^2
for (i in which(nevent>1)) {  # assume not too many ties
    denom <- fit$n.risk[i] - fit$n.event[i]*(0:(nevent[i]-1))/nevent[i] 
    risk2[i] <- mean(1/denom) # multiplier for the event
    ltemp[i] <- mean(1/denom^2)
    n2[i] <- mean(denom)
}

death <- (yindex <= tindex & rep(event, ntime))
term1 <- risk2[ff]
term2 <- lapply(fitrow, function(i) event[i]*ltemp[i])
term3 <- unlist(lapply(term2, cumsum))

sum1 <- c(0, term1)[ifelse(death, 1+yindex, 1)]
sum2 <- c(0, term3)[1 + pmin(yindex, tindex)]
if (ny==3) sum3 <- c(0, term3)[1 + pmin(startindex, tindex)]

if (ny==2) D <- matrix(sum1 -  sum2, ncol=ntime)
else       D <- matrix(sum1 + sum3 - sum2, ncol=ntime)

if (type=="pstate") D <- -D* c(0,fit$surv[ff])[1+ tindex]
else if (type=="auc") {
    <<auctrick>>
}
@

\paragraph{Kaplan-Meier}
For the Kaplan-Meier (a special case of the Aalen-Johansen) the underlying
algorithm is multiplicative, but we can turn it into an additive
algoritm with a slight of hand.

\begin{align*}
  S(t) &= \prod_{d_j\le t} (1- h(d_j)) \\
       &= \exp \left(\sum_{d_j\le t} \log(1- h(d_j)) \right) \\
       &= \exp \left(\sum_{d_j\le t} \log(r(d_j) - dN(d_j)) - \log(r(d_j)) \right) \\
  \frac{\partial S(t)}{\partial w_i} &= 
               S(t) \sum_{d_j\le t} \frac{Y_i(d_j) - dN_i(d_j)}{r(d_j) - dN(d_j)} -
                           \frac{Y_i(d_j)}{ r(d_j)}   
\end{align*}

The addend for term2 is now $1/n(n-e)$ where $e$ is the number of events, i.e.,
the same term as in the Greenwood variance, and term1 is $-1/n(n-e)$. 
The jumps in the KM curve are just a big larger than jumps in a FH estimate,
so it makes sense that these are just a bit larger.

<<residpart1-AJ>>=
death <- (yindex <= tindex & rep(event, ntime))
# dtemp avoids 1/0.  (When this occurs the influence is 0, since
#  the curve has dropped to zero; and this avoids Inf in term1 and term2).
dtemp <- ifelse(fit$n.risk==fit$n.event, 0, 1/(fit$n.risk- fit$n.event))
term1 <- dtemp[ff]
term2 <- lapply(fitrow, function(i) dtemp[i]*fit$n.event[i]/fit$n.risk[i])
term3 <- unlist(lapply(term2, cumsum))

add1 <- c(0, term1)[ifelse(death, 1+yindex, 1)]
add2 <- c(0, term3)[1 + pmin(yindex, tindex)]
if (ny==3) add3 <- c(0, term3)[1 + pmin(startindex, tindex)]

if (ny==2) D <- matrix(add1 -  add2, ncol=ntime)
else       D <- matrix(add1 + add3 - add2, ncol=ntime)

# survival is exp(-H) so the derivative is a simple transform of D
if (type== "pstate") D <- -D* c(1,fit$surv[ff])[1+ tindex]
else if (type == "auc") {
    <<auctrick>>
}
@

\subsection{Multi-state Aalen-Johansen estimate}
For multi-state models a correction for ties of similar spirit to the 
Efron approximation in a Cox model (the ctype=2 argument for \code{survfit})
is difficult: the `right' answer depends on the study.
Thus the ctype argument is not present.  
Both stype 1 and 2 are feasible, but currently only \code{stype=1} is
supported.
This makes the code somewhat simpler, but this is more than offset by the 
multi-state nature.
With multiple states we also need to account for influence on the starting
state $p(0)$.

One thing that can make this code slow is data that has been divided into a
very large number of intervals, giving a large number of observations for
each cluster.  We first deal with that by collapsing adjacent observations.

<<residuals.survfit>>=
rsurvpart2 <- function(Y, X, casewt, istate, times, cluster, type, fit,
                       method, collapse) {
    ny <- ncol(Y)
    ntime <- length(times)
    nstate <- length(fit$states)
    
    # ensure that Y, istate, and fit all use the same set of states
    states <- fit$states
    if (!identical(attr(Y, "states"), fit$states)) {
        map <- match(attr(Y, "states"), fit$states)
        Y[,ny] <- c(0, map)[1+ Y[,ny]]    # 0 = censored
        attr(Y, "states") <- fit$states
    }
    if (is.null(istate)) istate <- rep(1L, nrow(Y)) #everyone starts in s0
    else {
        if (is.character(istate)) istate <- factor(istate)
        if (is.factor(istate)) {
            if (!identical(levels(istate), fit$states)) {
                map <- match(levels(istate), fit$states)
                if (any(is.na(map))) stop("invalid levels in istate")
                istate <- map[istate]
            }       
        } # istate is numeric, we take what we get and hope it is right
    }

    # collapse redundant rows in Y, for efficiency
    #  a redundant row is a censored obs in the middle of a chain of times
    #  If the user wants individial obs, however, we would just have to
    #  expand it again
    if (ny==3 && collapse & any(duplicated(cluster))) {
        ord <- order(cluster, Y[,1])  # time within subject
        cfit <- .Call(Ccollapse, Y, X, istate, cluster, casewt, ord -1L) 
        if (nrow(cfit) < .8*length(X))  {
            # shrinking the data by 20 percent is worth it
            temp <- Y[ord,]
            Y <- cbind(temp[cfit[,1], 1], temp[cfit[2], 2:3])
            X <- X[cfit[,1]]
            istate <- istate[cfit[1,]]
            cluster <- cluster[cfit[1,]]
        }       
    }

    # Compute the initial leverage
    inf0 <- NULL
    if (is.null(fit$call$p0) && any(istate != istate[1])) { 
        #p0 was not supplied by the user, and the intitial states vary
        inf0 <- matrix(0., nrow=nrow(Y), ncol=nstate)
        i0fun <- function(i, fit, inf0) {
            # reprise algorithm in survfitCI
            p0 <- fit$p0
            t0 <- fit$time[1]
            if (ny==2) at.zero <- which(as.numeric(X) ==i)
            else       
                at.zero <- which(as.numeric(X) ==i &
                          (Y[,1] < t0 & Y[,2] >= t0))
            for (j in 1:nstate) {
                inf0[at.zero, j] <- (ifelse(istate[at.zero]==states[j], 1, 0) -
                                     p0[j])/sum(casewt[at.zero])
            }
            inf0
        }

        if (is.null(fit$strata)) inf0 <- i0fun(1, fit, inf0)
        else for (i in 1:length(levels(X)))
            inf0 <- i0fun(i, fit[i], inf0)  # each iteration fills in some rows
    }

    p0 <- fit$p0          # needed for method==1, type != cumhaz
    fit <- survfit0(fit)  # package the initial state into the picture
    start.time <- fit$time[1]

    # This next block is identical to the one in rsurvpart1, more comments are
    #  there
    etime <- (rowSums(fit$n.event) >0)
    event <- (Y[,ny] >0)
    # 
    #  Create a list whose first element contains the location of
    #   the death times in curve 1, second element for curve 2, etc.
    #  
    if (is.null(fit$strata)) fitrow <- list(which(etime))
    else {
        temp1 <- cumsum(fit$strata)
        temp2 <- c(1, temp1+1)
        fitrow <- lapply(1:length(fit$strata), function(i) {
            indx <- seq(temp2[i], temp1[i])
            indx[etime[indx]] # keep the death times
        }) 
    }
    ff <- unlist(fitrow)

    # for each time x, the index of the last death time which is <=x.
    #  0 if x is before the first death time
    matchfun <- function(x, fit, index) {
        dtime <- fit$time[index]  # subset to this curve
        i2 <- findInterval(x, dtime, left.open=FALSE)
        c(0, index)[i2 +1]
    }
     

    if (type== "cumhaz") {
        <<residpart2CH>>
    } else {
        <<residpart2AJ>>
    }   

    # since we may have done a partial collapse (removing redundant rows), the
    # parent routine can't collapse the data
    if (collapse & any(duplicated(cluster))) {
        if (length(dim(D)) ==2)
            D <- rowsum(D, cluster, reorder=FALSE)
        else { #rowsums has to be fooled
            dd <- dim(D)
            temp <- rowsum(matrix(D, nrow=dd[1]), cluster)
            D <- array(temp, dim=c(nrow(temp), dd[2:3]))
        }       
    }
    D
}
@ 

\paragraph{Nelson-Aalen}
The multi-state Nelson-Aalen estimate of the cumulative hazard at time $t$
is a vector with one element for each observed transition pair.  If there
were $k$ states there are potentially $k(k-1)$ transition pairs, though 
normally only a small number will occur in a given fit.  
We ignore transitions from state $j$ to state $j$.
Let $r(t)$ be the weighted number at risk at time $t$, in each state.
When some subject makes a $j:k$ transition, the $j:k$ transition will
have an increment of $w_i/r_j(t)$. 
This is precisely the same increment as the ordinary Nelson estimate.
The only change then is that we loop over the set of possible transitions,
creating a large output object.

<<residpart2CH>>=
# output matrix D will have one row per observation, one col for each
#  reporting time. tindex and yindex have the same dimension as D.
# tindex points to the last death time in fit which
#  is <= the reporting time.  (If there is only 1 curve, each col of
#  tindex will be a repeat of the same value.)
tindex <- matrix(0L, nrow(Y), length(times))
for (i in 1:length(fitrow)) {
    yrow <- which(as.integer(X) ==i)
    temp <- matchfun(times, fit, fitrow[[i]])
    tindex[yrow, ] <- rep(temp, each= length(yrow))
}
tindex[,] <- match(tindex, c(0,ff)) -1L  # the [,] preserves dimensions

# repeat the indexing for Y onto fit$time.  Each row of yindex points
#  to the last row of fit with death time <= Y[,ny]
ny <- ncol(Y)
yindex <- matrix(0L, nrow(Y), length(times))
event <- (Y[,ny] >0)
if (ny==3) startindex <- yindex
for (i in 1:length(fitrow)) {
    yrow <- (as.integer(X) ==i)  # rows of Y for this curve
    temp <- matchfun(Y[yrow,ny-1], fit, fitrow[[i]])
    yindex[yrow,] <- rep(temp, ncol(yindex))
    if (ny==3) {
        temp <- matchfun(Y[yrow,1], fit, fitrow[[i]])
        startindex[yrow,] <- rep(temp, ncol(yindex))
    }
}                    
yindex[,] <- match(yindex, c(0,ff)) -1L
if (ny==3) {
    startindex[,] <- match(startindex, c(0, ff)) -1L
    # no subtractions for report times before subject's entry
    startindex <- pmin(startindex, tindex) 
}

dstate <- Y[,ncol(Y)]
istate <- as.integer(istate)
ntrans <- ncol(fit$cumhaz)  # the number of possible transitions
D <- array(0, dim=c(nrow(Y), ntime, ntrans))

scount <- table(istate[dstate!=0], dstate[dstate!=0]) # observed transitions
state1 <- row(scount)[scount>0]
state2 <- col(scount)[scount>0]
temp <- paste(rownames(scount)[state1], 
              colnames(scount)[state2], sep='.')
if (!identical(temp, colnames(fit$cumhaz))) stop("setup error")

for (k in length(state1)) {
    e2 <- Y[,ny] == state2[k]
    add1 <- (yindex <= tindex & rep(e2, ntime))
    lsum <- unlist(lapply(fitrow, function(i) 
             cumsum(fit$n.event[i,k]/fit$n.risk[i,k]^2)))
    
    term1 <- c(0, 1/fit$n.risk[ff,k])[ifelse(add1, 1+yindex, 1)]
    term2 <- c(0, lsum)[1+pmin(yindex, tindex)]
    if (ny==3) term3 <- c(0, lsum)[1 + startindex]

    if (ny==2) D[,,k] <- matrix(term1 -  term2, ncol=ntime)
    else       D[,,k] <- matrix(term1 + term3 - term2, ncol=ntime)
}
@ 

\paragraph{Aalen-Johansen}
The multi-state AJ estimate is more complex.  Let $p(t)$ be the vector
of probability in state at time $t$.
Then
\begin{align}
  p(t) &= p(t-) [I+ A(t)] \nonumber \\
  \frac{\partial p(t)}{\partial w_i} &= \frac{\partial p(t-)}{\partial w_i} 
                                        [I+ A(t)]
     +  p(t-) \frac{\partial A(t)}{\partial w_i} \nonumber\\
   &= U_i(t-) [I+ A(t)] + p(t-) \frac{\partial A(t)}{\partial w_i} 
       \label{ajresidx2}
\end{align}

When we expand the left hand portion of \eqref{ajresidx2} to include all 
observations it becomes simple matrix multiplication, not so with
the right hand portion.
Each individual subject $i$ has a subject-specific
nstate * nstate derivative matrix $dA$, which will be non-zero only for the 
state (row) $j$ that the subject occupies at time $t-$. 
The $j$th row of $p(t-) dH$ is added to each subject's derivative.

The $A$ matrix at time $t$ has off diagonal elements and derivative
\begin{align}
A(t)_{jk} &= \frac{\sum_i w_i Y_{ij}(t) dN{ik}(t)}
     {\sum_i w_iY_{ij}(t)} \\
     &= \lambda_{jk}(t) \\
\frac{\partial A(t)}{\partial w_i} &= \frac{dN_{ik}(t) - \lambda_{jk}(t)}
     {\sum_i w_iY_{ij}(t)} \label{Aderiv}
\end{align}
    
This is the standard counting process notation: $Y_{ij}(t)$ is 1 if subject $i$
is in state $j$ and at risk at time $t-$, and $dN_{ik}(t)$ is a transition to
state $k$ at time $t$.
Each observation at risk appears in at most 1 row of $A(t)$, since they can
only be in one state.  
The diagonal element of $A$ are set so that each row sums to 0.
If there are no transitions out of state $j$ at some time point, then that
row of $A$ is zero.
Since the row sums are constant, the sum of the derivatives for each row
must be zero.

If we evaluate equation \eqref{ajresidx} directly there will be 
$O(nk^2)$ operations at each death time for the matrix product, and another
$O(nk)$  to add in the new increment.  For a large data set $d$ is often
of the same order as $n$, which makes this an expensive calculation.
But, this is what the C-code version currently does, because I have code that
actually works.


<<residpart2AJ>>=
if (method==1) {
    # Compute the result using the direct method, in C code
    # the routine is called separately for each curve, data in sorted order
    #
    is1 <- as.integer(istate) -1L  # 0 based subscripts for C
    if (is.null(inf0)) inf0 <- matrix(0, nrow=nrow(Y), ncol=nstate)
    if (all(as.integer(X) ==1)) { # only one curve
        if (ny==2) asort1 <- 0L else asort1 <- order(Y[,1], Y[,2]) -1L
        asort2 <- order(Y[,ny-1]) -1L
        tfit <- .Call(Csurvfitresid, Y, asort1, asort2, is1, 
                      casewt, p0, inf0, times, start.time, 
                      type== "auc")

        if (ntime==1) {
            if (type=="auc") D <- tfit[[2]] else D <- tfit[[1]]
        }
        else {
            if (type=="auc") D <- array(tfit[[2]], dim=c(nrow(Y), nstate, ntime))
            else         D <- array(tfit[[1]], dim=c(nrow(Y), nstate, ntime))
        }
    }
    else { # one curve at a time
        ix <- as.numeric(X)  # 1, 2, etc
        if (ntime==1) D <- matrix(0, nrow(Y), nstate)
        else D <- array(0, dim=c(nrow(Y), nstate, ntime))
        for (curve in 1:max(ix)) {
            j <- which(ix==curve)
            ytemp <- Y[j,,drop=FALSE]
            if (ny==2) asort1 <- 0L 
            else asort1 <- order(ytemp[,1], ytemp[,2]) -1L
            asort2 <- order(ytemp[,ny-1]) -1L

            # call with a subset of the data
            j <- which(ix== curve)
            tfit <- .Call(Csurvfitresid, ytemp, asort1, asort2, is1[j],
                          casewt[j], p0[curve,], inf0[j,], times, 
                          start.time, type=="auc")
            if (ntime==1) {
                if (type=="auc") D[j,] <- tfit[[2]] else D[j,] <- tfit[[1]]
            } else {
                if (type=="auc") D[j,,] <- tfit[[2]] else D[j,,] <- tfit[[1]]
            }
        }
    } 
    # the C code makes time the last dimension, we want it to be second
    if (ntime > 1) D <- aperm(D, c(1,3,2))
}
else {
    # method 2
    <<residpart2AJ2>>
}
@   

Can we speed this up?
An alternate is to look at the direct expansion.
\begin{align}
  p(t) &= p(0) \prod_{d_j \le t} [I+ A(d_j)] \nonumber \\
  \frac{\partial p(t)}{\partial w_i} &=
     \frac{\partial p(0)}{\partial w_i} \prod_{d_j \le t} [I+ A(d_j)] \\
     &  + p(0)\sum_{d_j \le t} \left( \prod_{k<j}[I+ A(d_k)]
            \frac{\partial A(d_j)}{\partial w_i}  
            \prod_{j<k, d_k\le t}[I+ A(d_k)]  \right)\nonumber \\
   &= \frac{\partial p(0)}{w_i} \prod_{d_j \le t} [I+ A(d_j)] +  
            p(d_{j-1}) \frac{\partial A(d_j)}{\partial w_i}
            \prod_{j<k, d_k\le t}[I+ A(d_k)] \label{ajresidy}
\end{align}
We cannot insert an $(I+ A(d_j))/(I + A(d_j))$ term and rearrange the last
equation so as to factor out $p(t)$, as was done in the KM case,
since matrix products do not commute.
Instead think of accumulating the terms sequentially.  
Let $B^{(j)}(t)$ be the nstate by nstate matrix derivative matrix with
row $j$ of $\lambda_{jk}/n_j(t)$, and zero in all of the other
rows, i.e., term 2 of equation \eqref{Aderiv} for someone in state $j$.
(This is the part of the derivative that is common to all subjects at
risk.) Let $B(t)$ be the sum of these matrices, i.e., all states filled.
Now, here is the trick.  The product $B^{(j)}(t)[I + A(t)]$ also is
zero for all but the $jth$ row, and is in fact equal to the $j$th
row of $B(t)[I + A(t)]$.
Further, $p(t-)B^{(j)}(t)[I + A(t)]$ is the $j$th row of
${\rm diag}(p(t-))B(t)[I + A(t)]$.

The key computation is based on a matrix of matrices.  Start with the following
definitions.  $T_{jk}$ is the $j$th term in the expansion, at
death time $k$.  $T_{jk}=0$ whenever $k=0$ or $j>k$.
Let $D(x)$ be the diagonal matrix.
\begin{align}
T_{01} &= D(p'(0))[I+ A(d_1)] & T_{02} &= T_{01}[I + A(d_2)] &
         T_{03} &= T_{02} [I + A(d_3)] &  \ldots \\
T_{11} &=  D(p(d_1)) B(d_1) & T_{12} &= T_{11}[I + A(d_2)] &
         T_{13} &= T_{12}[I + A(d_3)] & \ldots \\
T_{21} &=  0 & T_{22} &= D(p(d_2)) B(d_2) & T_{23} &= T_{22}[I+ A(d_2)] & \ldots \\
T_{31} &=  0 & T_{32}&=0 & T_{33} &= D(p(d_3)) B(d_3) &\ldots 
\end{align}
(According to the latex guide the above should be nicely spaced, but I get
equations that are touching.  Why?)

If $p(0)$ is a fixed value specified by the user then $p'(0)$ =0.
Otherwise $p(0)$ is the emprical distribution of the initial states, just
before the first death time $d_1$.  Let $n_0$ be the (weighted) count of 
subjects who are at risk at that time.  
The $j$th row of $p'(0)$ is defined as the deviative wrt $w_i$ for a subject
who starts in state $j$.  
If no one starts in state $j$ that row of the matrix will be 0, otherwise
it contains $(1-p_j(0)$ in the $jth$ element and $p_j(0)/n_0$ elsewhere.

Define the matrix $W_{jk} = \sum_{l=1}^j T_{lk}$, with $W_{j0}=0$.
Then for someone who enters at time $s$ such that $d_a < s \le d_{a+1}$,
is censored or has an event at time $t$ such that $d_b \le t <d_{b+1}$,
reporting at time $r$ such that $d_c \le r < d_{c+1}$, the first portion of
the contribution for an observation in state $j$ will be the 
$j$th row of $- (W_{br}-W_{ar})$. 

The second contribution is the effect of the $dN$ term in the derivative.
An observation that has a j:k transtion at time $d_i$ will have an
additional term of $c \prod_{k=i+1}^r [I + A(t_k)]$ where $c$ is a vector
with 
\begin{align*}
  c_j &= -1/n_j(d_i) \\
  c_k &=  1/n_j(d_i) \\
  c   &=  0 \;\mbox{otherwise}
\end{align*}

If there are multiple reporting times, it is currently simplest to do each
one separately (at least for now), having computed and stored the sets of
matrices $A(d_i)$ and $p(d_i)B(d_i)$ once at the start.
If there are multiple strata in a curve, this is done separately per stratum.

<<residpart2AJ2>>=
Yold <- Y
utime  <- fit$time[fit$time <= max(times) & etime] # unique death times
ndeath <- length(utime)    # number of unique event times
delta <- diff(c(start.time, utime))

# Expand Y
if (ny==2) split <- .Call(Csurvsplit, rep(0., nrow(Y)), Y[,1], times)
else split <- .Call(Csurvsplit, Y[,1], Y[,2], times)
X <- X[split$row]
casewt <- casewt[split$row]
istate <- istate[split$row]
Y <- cbind(split$start, split$end, 
            ifelse(split$censor, 0, Y[split$row,ny]))
ny <- 3

# Create a vector containing the index of each end time into the fit object
yindex <- ystart <- double(nrow(Y))
for (i in 1:length(fitrow)) {
    yrow <- (as.integer(X) ==i)  # rows of Y for this curve
    yindex[yrow] <- matchfun(Y[yrow, 2], fit, fitrow[[i]])
    ystart[yrow] <- matchfun(Y[yrow, 1], fit, fitrow[[i]])
}
# And one indexing the reporting times into fit
tindex <- matrix(0L, nrow=length(fitrow), ncol=ntime)
for (i in 1:length(fitrow)) {
    tindex[i,] <- matchfun(times, fit, fitrow[[i]])
}
yindex[,] <- match(yindex, c(0,ff)) -1L
tindex[,] <- match(tindex, c(0,ff)) -1L
ystart[,] <- pmin(match(ystart, c(0,ff)) -1L, tindex)

# Create the array of C matrices
cmat <- array(0, dim=c(nstate, nstate, ndeath)) # max(i2) = ndeath, by design
Hmat <- cmat

# We only care about observations that had a transition; any transitions
#  after the last reporting time are not relevant
transition <- (Y[,ny] !=0 & Y[,ny] != istate &
               Y[,ny-1] <= max(times)) # obs that had a transition
i2 <- match(yindex, sort(unique(yindex)))  # which C matrix this obs goes to
i2 <- i2[transition]
from <- as.numeric(istate[transition])  # from this state
to   <- Y[transition, ny]   # to this state
nrisk <- fit$n.risk[cbind(yindex[transition], from)]  # number at risk
wt <- casewt[transition]
for (i in seq(along.with =from)) {
    j <- c(from[i], to[i])
    haz <- wt[i]/nrisk[i]
    cmat[from[i], j, i2[i]] <- cmat[from[i], j, i2[i]] + c(-haz, haz)
}
for (i in 1:ndeath) Hmat[,,i] <- cmat[,,i] + diag(nstate)

# The transformation matrix H(t) at time t  is cmat[,,t] + I
# Create the set of W and V matrices.
# 
dindex <- which(etime & fit$time <= max(times))
Wmat <- Vmat <- array(0, dim=c(nstate, nstate, ndeath))
for (i in ndeath:1) {
    j <- match(dindex[i], tindex, nomatch=0) 
    if (j > 0) {
        # this death matches one of the reporting times
        Wmat[,,i] <- diag(nstate)
        Vmat[,,i] <- matrix(0, nstate, nstate)
    } 
    else {
        Wmat[,,i] <- Hmat[,,i+1] %*% Wmat[,,i+1]
        Vmat[,,i] <- delta[i] +  Hmat[,,i+1] %*% Wmat[,,i+1]
    }
}
@

The above code has created the Wmat array for all reporting times and
for all the curves (if more than one). 
Each of them reaches forward to the next reporting time.
Now work forward in time.

<<residpart2AJ2>>=
iterm <- array(0, dim=c(nstate, nstate, ndeath)) # term in equation
itemp <- vtemp <- matrix(0, nstate, nstate)  # cumulative sum, temporary
isum  <- isum2 <- iterm  # cumulative sum
vsum  <- vsum2 <- vterm <- iterm
for (i in 1:ndeath) {
    j <- dindex[i]
    n0 <- ifelse(fit$n.risk[j,] ==0, 1, fit$n.risk[j,]) # avoid 0/0
    iterm[,,i] <- ((fit$pstate[j-1,]/n0) * cmat[,,i]) %*% Wmat[,,i]
    vterm[,,i] <- ((fit$pstate[j-1,]/n0) * cmat[,,i]) %*% Vmat[,,i]
    itemp <- itemp + iterm[,,i]
    vtemp <- vtemp + vterm[,,i]
    isum[,,i] <- itemp
    vsum[,,i] <- vtemp
    j <- match(dindex[i], tindex, nomatch=0)
    if (j>0) itemp <- vtemp <- matrix(0, nstate, nstate)  # reset
    isum2[,,i] <- itemp
    vsum2[,,i] <- vtemp
}

# We want to add isum[state,, entry time] - isum[state,, exit time] for
#  each subject, and for those with an a:b transition there will be an 
#  additional vector with -1, 1 in the a and b position.
i1 <- match(ystart, sort(unique(yindex)), nomatch=0) # start at 0 gives 0
i2 <- match(yindex, sort(unique(yindex)))
D <- matrix(0., nrow(Y), nstate)
keep <- (Y[,2] <= max(times))  # any intervals after the last reporting time
                                # will have 0 influence
for (i in which(keep)) {
    if (Y[i,3] !=0 && istate[i] != Y[i,3]) {
        z <- fit$pstate[yindex[i]-1, istate[i]]/fit$n.risk[yindex[i], istate[i]]
        temp <- double(nstate)
        temp[istate[i]] = -z
        temp[Y[i,3]]    =  z
        temp <- temp %*% Wmat[,,i2[i]] - isum[istate[i],,i2[i]]
        if (i1[i] >0) temp <- temp + isum2[istate[i],, i1[i]]
        D[i,] <- temp
    }
    else {
        if (i1[i] >0) D[i,] = isum2[istate[i],,i1[i]] - isum[istate[i],, i2[i]]
        else  D[i,] =  -isum[istate[i],, i2[i]]
    }
}
@

By design, each row of $Y$, and hence each row of $D$, corresponds to a unique
curve, and also to a unique period in the reporting intervals.
(Any Y intervals after the last reporting time will have D=0 for the row.)
If there are multiple reporting intervals, create an array with one
n by nstate slice for each.
If a row lies in the first interval, $D$ currently contains its influence
on that interval.  It's influence on the second interval is the vector times
$\prod H(d_k)$ where $k$ is the set of event times $>$ the first reporting time
and $\le$ the second one.  
 
<<residpart2AJ2>>=
Dsave <- D
if (!is.null(inf0)) {
    # add in the initial influence, to the first row of each obs
    #   (inf0 was created on unsplit data)
    j <- which(!duplicated(split$row))
    D[j,] <- D[j,] + (inf0%*% Hmat[,,1] %*% Wmat[,,1])
}
if (ntime > 1) {
    interval <- findInterval(yindex, tindex, left.open=TRUE)
    D2 <- array(0., dim=c(dim(D), ntime))
    D2[interval==0,,1] <- D[interval==0,]
    for (i in 1:(ntime-1)) {
        D2[interval==i,,i+1] = D[interval==i,]
        j <- tindex[i]
        D2[,,i+1] = D2[,,i+1] + D2[,,i] %*% (Hmat[,,j] %*% Wmat[,,j])
    } 
    D <- D2
}

# undo any artificial split
if (any(duplicated(split$row))) {
    if (ntime==1) D <- rowsum(D, split$row)
    else {
        # rowsums has to be fooled
        temp <- rowsum(matrix(D, ncol=(nstate*ntime)), split$row)
        # then undo it
        D <- array(temp, dim=c(nrow(temp), nstate, ntime))
    }
}
@
