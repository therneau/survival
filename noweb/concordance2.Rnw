\subsection{Concordance}
 The concordance statistic is gaining popularity as a measure of goodness-of-fit
in survival models.  
In general let $y$ and $\hat y$ be observed and predicted data values.
A pair of observations $i$, $j$ is considered condordant if either
$y_i > y_j, \hat y_i > \hat y_j$ or $y_i < y_j, \hat y_i < \hat y_j$.
For a Cox model remember that the predicted survival $hat y$ is longer if
the risk score $X\beta$ is lower.
The concordance $C$ is the fraction of concordant pairs.

One wrinkle is what to do with ties in either $y$ or $\hat y$.  Such pairs
can be ignored in the count (treated as incomparable), treated as discordant,
or given a score of 1/2.
\begin{itemize}
  \item Kendall's $\tau$-a scores ties as 0.
  \item Kendall's $\tau$-b and the Goodman-Kruskal $\gamma$ ignore ties in 
    either $y$ or $\hat y$.
  \item Somers' $d$ treats ties in $y$ as incomparable, pairs that are not
    ties in $y$ but are tied in $\hat y$ score as 1/2.
\end{itemize}
All three of the above range from -1 to 1, while $C$ ranges from 0 to 1;
$C = (\tau +1)/2$.  
For logistic regression $C$ is computed using the Somers' choices, this leads
to the AUC, and this will be the default for the concordance measure.
For survival data any pairs which cannot be ranked with certainty are
considered incomparable.
For instance $s_i$ is censored at time 10 and $s_j$ is an event (or censor) 
at time 20.  Subject $i$ may or may not survive longer than subject $j$.  
Note that if $s_i$ is censored at time
10 and $s_j$ is an event at time 10 then $s_i > s_j$.  
Observations that are in different strata are also incomparable, 
since the Cox model only compares within strata.

The program creates 4 variables, which are the number of concordant pairs, 
discordant, tied on time, and tied on $x$ but not on time.  
The default concordance is 
based on the AUC definition, but all 4 values are reported back so that a user
can recreate the others if desired.

The primary compuational questions is how to do this efficiently, i.e., better
that the naive $O(n^2)$ algorithm that loops across all $n(n-1)/2$ 
possible pairs.
There are two key ideas.
\begin{enumerate}
\item Rearrange the counting so that we do it by death times.
  For each death we count the number of other subjects in the risk set whose
  score is higher, lower, or tied and add it into the totals.
This also neatly solves the question of time-dependent covariates.
\item To count the number higher and lower we need to rank the subjects
  in the risk set by their scores $r_i$.  This can be done in $O(\log n)$
  time if the data is kept in a binary tree.
\end{enumerate}

\begin{figure}
  \myfig{balance}
  \caption{A balanced tree of 13 nodes.}
  \label{treefig}
\end{figure}

Figure  \ref{treefig} shows a balanced binary tree containing  
13 risk scores.  For each node the left child and all its descendants
have a smaller value than the parent, the right child and all its
descendents have a larger value.
Each node in figure \ref{treefig} is also annotated with the total weight
of observations in that node and the weight for all its children 
(not shown on graph).  
Assume that the tree shown represents all of the subjects still alive at the
time a particular subject ``Smith'' expires, and that Smith has the risk score
2.1.  The concordant pairs are all of those with a risk score greater than
2.1, which can be found by traversing the tree from the top down,
adding the (parent - child) value each time we branch left (5-3 at the 2.6
node),
with a last addition of the right hand child when we 
find the node with Smith's value (1).      %'
There are 3 concordant and 12-3=9 discordant pairs.
This takes a little less than $\log_2(n)$ steps on average, as compared to an
average of $n/2$ for the naive method.  The difference can matter when $n$ is
large since this traversal must be done for each event.
(In the code below we start at Smith's node and walk up.)             %'

The classic way to store trees is as a linked list.  There are several 
algorithms for adding and subtracting nodes from a tree while maintaining
the balance (red-black trees, AA trees, etc) but we take a different 
approach.  Since we need to deal with case weights in the model and we
know all the risk score at the outset, the full set of risk scores is
organised into a tree at the beginning and node counts are changed to zero
as observations are removed.  

If we index the nodes of the tree as 1 for the top, 2--3 for the next 
horizontal row, 4--7 for the next, \ldots then the parent-child 
traversal becomes particularly easy.
The parent of node $i$ is $i/2$ (integer arithmetic) and the children of
node $i$ are $2i$ and $2i +1$.  In C code the indices start at 0 and the
children are $2i+1$ and $2i+2$ and the parent is $(i-1)/2$.  The following
bit of code returns the indices of a sorted list when placed into such a
tree.  
The basic idea is that the rows of the tree start at indices 1, 2, 4, \ldots.
For the above tree, the last row will contains the 1st, 3rd, \ldots, 11th
smallest ranks.  The next row above contains every other value of the
ranks \emph{not yet assigned}, and etc to the top of the tree.  There is
some care to make sure the result is an integer.
<<btree>>=
btree <- function(n) {
    ranks <- rep(0L, n)  #will be overwritten
    yet.to.do <- 1:n
    depth <- floor(logb(n,2))
    start <- as.integer(2^depth)
    lastrow.length <- 1+n-start
    indx <- seq(1L, by=2L, length= lastrow.length)
    ranks[yet.to.do[indx]] <- start + 0:(length(indx)-1L)
    yet.to.do <- yet.to.do[-indx]

    while (start >1) {
        start <- as.integer(start/2)
        indx <- seq(1L, by=2L, length=start)
        ranks[yet.to.do[indx]] <- start + 0:(start-1L)
        yet.to.do <- yet.to.do[-indx]
    }
    ranks
}
@ 
Referring again to figure \ref{treefig}, [[btree(13)]] yields the vector
[[8  4  9  2 10  5 11  1 12  6 13  3  7]] meaning that the smallest element
will be in position 8 of the tree, the next smallest in position 4, etc.

Here is a shorter recursive version.
It knows the form of trees with 1, 2, or 3 nodes; and builds the
others from them.  
The maximum depth of recursion is $\log_2(n) -1$.
It is more clever but a bit slower.  (Not that it matters as both
take less than 5 seconds for a million elements.)
<<btree2>>=
btree <- function(n) {
   tfun <- function(n, id, power) {
       if (n==1) id
       else if (n==2) c(2L *id, id)
       else if (n==3) c(2L*id, id, 2L*id +1L)
       else {
           nleft <- if (n== power*2) power  else min(power-1, n-power/2)
           c(tfun(nleft, 2L *id, power/2), id,
             tfun(n-(nleft+1), 2L*id +1L, power/2))
           }
       }
   tfun(n, 1L, 2^(floor(logb(n-1,2))))
   }
@ 

The next question is how to compute a variance for the result.
One approach is to compute an infinitesimal jackknife (IJ) estimate,
for which we need derivatives with respect to the weights.
For each event the two numbers of interest are the 
numerator $C-D$ and the denominator $C+D + T$,
which can be written directly in terms of the weights.

\begin{align*}
  C -D  &= \sum_i w_i \delta_i \sum_{t_j > t_i} w_j \rm{sign}(r_i - r_j) \\
  C +D+T &= \sum_i w_i \delta_i \sum_{t_j > t_i} w_j 
 \end{align*}

The first derivatives of these quantities with respect to an arbitrary subject
$k$ are
\begin{align}
  \frac{\partial C-D}{\partial w_k} &= 
         \sum_{t_j > t_k} w_j \delta_k \rm{sign}(r_k - r_j) -
          \sum_{t_j < t_k} w_j \delta_j \rm{sign}(r_k - r_j) \label{derivC} \\
  \frac{\partial C+D+T}{\partial w_k} &= \sum_{t_j > t_k} w_j \delta_k +
                                \sum_{t_j < t_k} w_j \delta_j  \label{derivT}\\
\end{align}
We need to keep two vectors of derivatives each of length $n$, the number
of observations.  The parent routine in R will worry about data sets with
multiple rows per subject, for which the person has to be added back 
together.

I had originally avoided this approach because it appears to be an $O(nd)$
computation, adding a bit to each at-risk subject each time there is an
event.  A key insight due to David Watson is that we can avoid this.
Within the tree we keep the sum of weights for each node and its left and right
children, along with a parallel triple that totals only the deaths.   
The tree is updated as each subject is added or deleted, containing
the current cumulative totals.  We only need to update derivative values for
each subject when the subject enters and when they leave.
(For ordinary Cox models everyone enters at 0, or rather leaves at zero since
our code will go from longest to shortest time.)

A second variance estimate is based on the Cox model.  
The insight used here is to consider a Cox model with time dependent
covariates, where the covariate $x$ at each death time has been
transformed into ${\rm rank}(x)$. 
It is easy to show that the Cox score statistic contribution at each death
is $(D-C)/2$ where $C$ and $D$ are the number of concordant
and discordant pairs contributed at that death time (for a Cox fit using the
Breslow approximation).
The contribution to the variance of the score statistic is 
$V(t) =\sum (r_i - \overline{r})^2 /n$, the $r_i$ being the ranks at
that time point and $n$ the number at risk.  
How can we update this sum using an update formula?  
First remember the identity
\begin{equation*} \sum w_i(x_i - \overline{x})^2 = \sum w_i(x_i-c)^2 - 
    \sum w_i(c - \overline{x})^2
\end{equation*}
true for any set of values $x$ and centering constant $c$.
For weighted data define the rank of an observation with risk score $r_k$
as
\begin{equation*}
  {\rm rank} = \sum_{r_i<r_k} w_i + (1/2)\sum_{r_i=r_k} w_i
\end{equation*}
These correspond to the midpoints of the rise on an empirical CDF, and
for unweighted data without ties gives ranks of .5, 1.5, \ldots, $n-.5$.
   
Assume we have just added obs $k$ to the tree.
Since the mean rank = $\mu_g = \sum(w_i)/2$ the grand mean increases by
    $w_k/2$.
We can divide the subjects currently in the tree into 3 groups.
\begin{enumerate}
  \item Those with risk score lower than the new addition.  Their rank will
    not change.
  \item Those tied with the new addition.  Their rank will increase by
    $w_k/2$.
  \item Those above the new addition.  Their rank increases by $w_k$.
\end{enumerate}


Let $\mu_\ell$ be the mean rank for all observations currently in the
tree of rank lower than $r_k$, the item we are about to add,
$\mu_u$ be the mean for all those above in rank (after the addition), 
$\mu_g$ the grand mean, and $\mu_n$ the new grand mean after adding
in subject $k$.
We have 
\begin{align*}
  \mu_\ell &= \sum_{r_i<r_k} w_i/2 \\
  \mu_u & =  \sum_{r_i<=r_k} w_i + \sum_{r_i \ge r_k} w_i/2
\end{align*}  

For items of lower rank than $r_k$, none of their ranks will change
with the addition of this new observation.  This leads to the 
update formula on the third line below.  (I'm using $i<k$ as shorthand  %'
for $r_i < r_k$ below)
\begin{align}
   \sum_{i<k} w_i(r_i - \mu_g)^2 &= \sum_{i<k} w_i(r_i - \mu_\ell)^2 +
      (\sum_{i<k} w_i)(\mu_\ell - \mu_g)^2 \nonumber \\
   \sum_{i<k} w_i(r_i - \mu_n)^2 &= \sum_{i<k} w_i(r_i - \mu_\ell)^2 +
      (\sum_{i<k} w_i)(\mu_\ell - \mu_n)^2 \nonumber \\
   \sum_{i<k} w_i(r_i - \mu_n)^2 -   \sum_{i<k} w_i(r_i - \mu_g)^2 &=
    (\sum_{i<k} w_i) [(\mu_\ell - \mu_n)^2 - (\mu_\ell - \mu_g)^2] 
              \nonumber \\
   &=  (\sum_{i<k} w_i)(\mu_n+ \mu_g - 2\mu_{\ell})(\mu_n - \mu_g) 
                    \label{lower1} \\
   &=  (\sum_{i<k} w_i)(\mu_n+ \mu_g - 2\mu_{\ell}) w_k/2 \label{lower}
\end{align}

For items of larger rank than $r_k$, all of the ranks increase by $w_k$
when we add the new item and $\mu_u$ increases by $w_k$, 
thus the sum of squares within the group
is unchanged.  The same derivation as above gives an update of
\begin{align}
  \sum_{i>k} w_i(r_i - \mu_n)^2 -   \sum_{i>k} w_i(r_i - \mu_g)^2 &=
    (\sum_{i>k} w_i) [(\mu_u -\mu_n)^2 - ((\mu_u-w_k) - \mu_g)^2] \nonumber \\
  &= (\sum_{i>k} w_i) (\mu_n + z - 2\mu_u)(\mu_n -z) \label{upper1} \\
  &= (\sum_{i>k} w_i) (\mu_n+z - 2\mu_u) (-w_k/2) \label{upper}\\
  z&\equiv \mu_g+ w_k \nonumber
\end{align}

For items of tied rank, their rank increases by the same amount as the
overall mean, and so their contribution to the total SS is unchanged.
The final part of the update step is to add in the SS contributed by
the new observation.

An observation is removed from the tree whenver the current time becomes
less than the (start, stop] interval of the datum.
The ranks for observations of lower risk are unchanged by the removal
so equation \eqref{lower1} applies just as before, but with the new mean
smaller than the old so the last term in equation \eqref{lower} changes
sign.
For the observations of higher risk both the mean and the ranks change
by $w_k$ and equation \eqref{upper1} holds but with $z=\mu_0- w_k$.


We can now define the C-routine that does the bulk of the work.
First we give the outline shell of the code and then discuss the
parts one by one.  This routine  is for ordinary survival data, and
will be called once per stratum.
Input variables are
\begin{description}
  \item[n] the number of observations
  \item[y] matrix containing the time and status, data is sorted by ascending 
    time, with deaths preceding censorings.
  \item[indx] the tree node at which this observation's risk score resides  %'
  \item[wt] case weight for the observation
  \item[sum] scratch space, weights for each node of the tree: 
    3 values are for the node, all left children, and all right children
  \item[count] the returned counts of concordant, discordant, tied on x, 
    tied on time, and the variance
\end{description}

<<concordance1>>=
#include "survS.h"
SEXP concordance1(SEXP y, SEXP wt2,  SEXP indx2, SEXP ntree2) {
    int i, j, k, index;
    int child, parent;
    int n, ntree;
    double *time, *status;
    double *twt, *nwt, *count;
    double vss, myrank, wsum1, wsum2, wsum3; /*sum of wts below, tied, above*/
    double lmean, umean, oldmean, newmean;
        
    double ndeath;   /* weighted number of deaths at this point */
    
    SEXP count2;
    double *wt;
    int    *indx;
    
    n = nrows(y);
    ntree = asInteger(ntree2);
    wt = REAL(wt2);
    indx = INTEGER(indx2);
    
    time = REAL(y);
    status = time + n;
    PROTECT(count2 = allocVector(REALSXP, 5));
    count = REAL(count2);  /* count5 contains the information matrix */
    twt = (double *) R_alloc(2*ntree, sizeof(double));
    nwt = twt + ntree;
    for (i=0; i< 2*ntree; i++) twt[i] =0.0;
    for (i=0; i<5; i++) count[i]=0.0;
    vss=0;

    <<concordance1-work>>
        
    UNPROTECT(1);
    return(count2);
}
@ 
The key part of our computation is to update the vectors of weights.
We don't actually pass the risk score values $r$ into the routine,   %'
it is enough for each observation to point to the appropriate tree
node.
The tree contains the  for everyone whose survival is larger
than the time currently under review, so starts with all weights
equal to zero.  
For any pair of observations $i,j$ we need to add [[wt[i]*wt[j]]]
to the appropriate count.
Starting at the largest time (which is sorted last), walk through the tree.
\begin{itemize}
  \item If it is a death time, we need to process all the deaths tied at
    this time.
    \begin{enumerate}
      \item Add [[wt[i] * wt[j]]] to the tied-on-time total, 
	for all pairs $i,j$ of tied times.
      \item The addition to tied-on-r will be the weight of this 
        observation times
        the sum of weights for all others with the same risk score and a
        a greater time, i.e., the weight found at [[indx[i]]] in the tree.
      \item Similarly for those with smaller or larger risk scores.  First add
        in the children of this node.  The left child will be smaller risk 
        scores (and longer times) adding to the concordant pairs, 
        the right child discordant.
        Then walk up the tree to the root. 
        At each step up we add in data for the 'not me' branch.
        If we were the right branch (even number node) of a parent
        then when moving up we add in the left branch counts, and vice-versa. 
    \end{enumerate}
    \item Now add this set of subject weights into the tree. The weight for
      a node is [[nwt]] and for the node and all its children is [[twt]].
\end{itemize}
<<concordance1-work>>=
for (i=n-1; i>=0; ) {
    ndeath =0;
    if (status[i]==1) { /* process all tied deaths at this point */
        for (j=i; j>=0 && status[j]==1 && time[j]==time[i]; j--) {
            ndeath += wt[j];
            index = indx[j];
            for (k=i; k>j; k--) count[3] += wt[j]*wt[k]; /* tied on time */
            count[2] += wt[j] * nwt[index];              /* tied on x */
	    child = (2*index) +1;  /* left child */
	    if (child < ntree)
		count[0] += wt[j] * twt[child];  /*left children */
	    child++;
	    if (child < ntree)
                count[1] += wt[j] * twt[child]; /*right children */
	    
            while (index >0) {  /* walk up the tree  */
		parent = (index-1)/2;
		if (index & 1)   /* I am the left child */
		    count[1] += wt[j] * (twt[parent] - twt[index]);
		else count[0] += wt[j] * (twt[parent] - twt[index]);
		index = parent;
		}
	    }
	}                    
    else j = i-1;
    
    /* Add the weights for these obs into the tree and update variance*/
    for (; i>j; i--) {
        wsum1=0; 
        oldmean = twt[0]/2;
	index = indx[i];
	nwt[index] += wt[i];
        twt[index] += wt[i];
        wsum2 = nwt[index];
        child = 2*index +1;  /* left child */
        if (child < ntree) wsum1 += twt[child];

	while (index >0) {
	    parent = (index-1)/2;
            twt[parent] += wt[i];
            if (!(index&1)) /* I am a right child */
                wsum1 += (twt[parent] - twt[index]);
            index=parent;
            }
        wsum3 = twt[0] - (wsum1 + wsum2); /* sum of weights above */
        lmean = wsum1/2;
        umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
        newmean = twt[0]/2;
        myrank = wsum1 + wsum2/2;
        vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean - oldmean);
        vss += wsum3*(newmean+ oldmean+ wt[i]- 2*umean) *(oldmean-newmean);
        vss += wt[i]* (myrank -newmean)*(myrank -newmean);
	}
    count[4] += ndeath * vss/twt[0];
    }
@ 

The code for [start, stop) data is quite similar.  
As in the agreg routines there are two sort indices, the first indexes
the data by stop time, longest to earliest, and the second by start time. 
The [[y]] variable now has three columns.
<<concordance1>>= 
SEXP concordance2(SEXP y,     SEXP wt2,  SEXP indx2, SEXP ntree2,
                  SEXP sortstop, SEXP sortstart) {
    int i, j, k, index;
    int child, parent;
    int n, ntree;
    int istart, iptr, jptr;
    double *time1, *time2, *status, dtime;
    double *twt, *nwt, *count;
    int *sort1, *sort2;
    double vss, myrank;
    double wsum1, wsum2, wsum3; /*sum of wts below, tied, above*/
    double lmean, umean, oldmean, newmean;
    double ndeath;
    SEXP count2;
    double *wt;
    int    *indx;
    
    n = nrows(y);
    ntree = asInteger(ntree2);
    wt = REAL(wt2);
    indx = INTEGER(indx2);
    sort2 = INTEGER(sortstop);
    sort1 = INTEGER(sortstart);
    
    time1 = REAL(y);
    time2 = time1 + n;
    status= time2 + n;
    PROTECT(count2 = allocVector(REALSXP, 5));
    count = REAL(count2);
    twt = (double *) R_alloc(2*ntree, sizeof(double));
    nwt = twt + ntree;
    for (i=0; i< 2*ntree; i++) twt[i] =0.0;
    for (i=0; i<5; i++) count[i]=0.0;
    vss =0;
    <<concordance2-work>>
        
    UNPROTECT(1);
    return(count2);
}
@ 

The processing changes in 2 ways
\begin{itemize}
  \item The loops go from $0$ to $n-1$ instead of $n-1$ to 0.  We need
    to use [[sort1[i]]] instead of [[i]] as the subscript for the time2 and wt
    vectors.  (The sort vectors go backwards in time.)
    This happens enough that we use a temporary variables [[iptr]] and [[jptr]]
    to avoid the double subscript.
  \item As we move from the longest time to the shortest observations are added
    into the tree of weights whenever we encounter their stop time. 
    This is just as before.  Weights now also need to be removed from the 
    tree whenever we encounter an observation's start time.              %'
    It is convenient ``catch up'' on this second task whenever we encounter 
    a death.
\end{itemize}

<<concordance2-work>>=
istart = 0;  /* where we are with start times */
for (i=0; i<n; ) {
    iptr = sort2[i];  /* In  reverse death time order */
    ndeath =0;
    if (status[iptr]==1) {
	/* Toss people out of the tree  and update variance */
	dtime = time2[iptr];
	for (; istart < n && time1[sort1[istart]] >= dtime; istart++) {
            wsum1 =0;
	    oldmean = twt[0]/2;
	    jptr = sort1[istart];
	    index = indx[jptr];
	    nwt[index] -= wt[jptr];
            twt[index] -= wt[jptr];
            wsum2 = nwt[index];
            child = 2*index +1;  /* left child */
	    if (child < ntree) wsum1 += twt[child];
	    while (index >0) {
		parent = (index-1)/2;
		twt[parent] -= wt[jptr];
		if (!(index&1)) /* I am a right child */
		    wsum1 += (twt[parent] - twt[index]);
		index=parent;
		}
	    wsum3 = twt[0] - (wsum1 + wsum2);
	    lmean = wsum1/2;
	    umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
	    newmean = twt[0]/2;
	    myrank = wsum1 + wsum2/2;
            vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean-oldmean);
            oldmean -= wt[jptr];  /* the z in equations above */
            vss += wsum3*(newmean+ oldmean -2*umean) * (newmean-oldmean);
	    vss -= wt[jptr]* (myrank -newmean)*(myrank -newmean);
            }
	    
	/* Process deaths */
	for (j=i; j <n && status[sort2[j]]==1 && time2[sort2[j]]==dtime; j++) {
	    jptr =  sort2[j];
	    ndeath += wt[jptr];
            index = indx[jptr];
            for (k=i; k<j; k++) count[3] += wt[jptr]*wt[sort2[k]]; 
            count[2] += wt[jptr] * nwt[index];            /* tied on x */
            child = (2*index) +1;   /* left child */
            if (child < ntree) count[0] += wt[jptr] * twt[child];
            child++;
            if (child < ntree) count[1] += wt[jptr] * twt[child];

            while (index >0) {  /* walk up the tree  */
                parent = (index-1)/2;
                if (index &1)   /* I am the left child */
                     count[1] += wt[jptr] * (twt[parent] - twt[index]);
                else count[0] += wt[jptr] * (twt[parent] - twt[index]);
                index = parent;
                }
	    }                    
        }
    else j = i+1;

    /* Add the weights for these obs into the tree and compute variance */
    for (; i<j; i++) {
        wsum1 =0;
        oldmean = twt[0]/2;
        iptr = sort2[i];
	index = indx[iptr];
        nwt[index] += wt[iptr];
	twt[index] += wt[iptr];
        wsum2 = nwt[index];
        child = 2*index +1;  /* left child */
        if (child < ntree) wsum1 += twt[child];
	while (index >0) {
            parent = (index-1)/2;
            twt[parent] += wt[iptr];
            if (!(index&1)) /* I am a right child */
                wsum1 += (twt[parent] - twt[index]);
            index=parent;
	    }
        wsum3 = twt[0] - (wsum1 + wsum2);
        lmean = wsum1/2;
        umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
        newmean = twt[0]/2;
        myrank = wsum1 + wsum2/2;
        vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean-oldmean);
        vss += wsum3*(newmean+ oldmean +wt[iptr] - 2*umean) * (oldmean-newmean);
        vss += wt[iptr]* (myrank -newmean)*(myrank -newmean);
        }
    count[4] += ndeath * vss/twt[0];
    }
@ 

One last wrinkle is tied risk scores: they are all set to point to
the same node of the tree.
Here is the main routine.
<<concordance>>=
concordance <- function(x, ...) 
    UseMethod("concordance")

concordance.formula <- function(formula, data,
                            weights, subset, na.action,
                            ties="ih", timewt=c("n", "S", "S/G") {
    Call <- match.call()  # save a copy of of the call, as documentation
    timewt <- match.arg(timewt)

    index <- match(c("formula", "data", "weights", "subset", "na.action", "id"),
                   names(Call), nomatch=0)
    temp <- Call[c(1, index)]
    temp[[1L]] <-  quote(stats::model.frame)
    special <- c("strata", "cluster")
    temp$formula <- if(missing(data)) terms(formula, special
                    else              terms(formula, special, data=data)
    mf <- eval(temp, parent.frame())  # model frame
    if (nrow(mf) ==0) stop("No (non-missing) observations")
    Terms <- terms(mf)

    Y <- model.extract(mf, "response")
    if (!inherits(Y, "Surv")) {
        if (is.numeric(Y) && is.vector(Y))  Y <- Surv(Y)
        else stop("left hand side of the formula  must be a numeric vector or a surival")
    }
    n <- nrow(Y)

    wt <- model.extract(m, 'weights')
    offset<- attr(Terms, "offset")
    if (length(offset)>0) stop("Offset terms not allowed")

    stemp <- untangle.specials(Terms, 'strata')
    if (length(stemp$vars)) {
	if (length(stemp$vars)==1) strat <- m[[stemp$vars]]
	else strat <- strata(m[,stemp$vars], shortlabel=TRUE)
        Terms <- Terms[-stemp$terms]
    }
    else strat <- NULL
    
    x <- model.matrix(Terms, m)[,-1, drop=FALSE]  #remove the intercept
    if (ncol(x) > 1) stop("Only one predictor variable allowed")

    count <- survConcordance.fit(Y, x, strat, wt)
    if (is.null(strat)) {
        concordance <- (count[1] + count[3]/2)/sum(count[1:3])
        std.err <- count[5]/(2* sum(count[1:3]))
        }
    else {
        temp <- colSums(count)
        concordance <- (temp[1] + temp[3]/2)/ sum(temp[1:3])
        std.err <- temp[5]/(2*sum(temp[1:3]))
        }

    fit <- list(concordance= concordance, stats=count, n=n, 
                std.err=std.err, call=Call)
    na.action <- attr(m, "na.action")
    if (length(na.action)) fit$na.action <- na.action

    oldClass(fit) <- 'survConcordance'
    fit
}

print.survConcordance <- function(x, ...) {
    if(!is.null(cl <- x$call)) {
        cat("Call:\n")
        dput(cl)
        cat("\n")
        }
    omit <- x$na.action
    if(length(omit))
        cat("  n=", x$n, " (", naprint(omit), ")\n", sep = "")
    else cat("  n=", x$n, "\n")
    cat("Concordance= ", format(x$concordance), " se= ", format(x$std.err),
        '\n', sep='')
    print(x$stats)

    invisible(x)
    }
@ 

This part of the compuation is a separate function, since it is
also called by the coxph routines. 
Although we are very careful to create integers and/or doubles for the
arguments to .Call I still wrap them in the appropriate as.xxx 
construction: ``belt and suspenders''.
Also, referring to the the mathematics many paragraphs ago, the C routine
returns the variance of $(C-D)/2$ and we return the standard deviation of
$(C-D)$.
If this routine is called with all the x values identical, then $C$ and $D$
will both be zero, but the calculated variance of $C-D$ can be a nonzero
tiny number due to round off error.  Since this can cause a warning message
from the sqrt function we check and correct this.
<<survConcordance.fit>>=
survConcordance.fit <- function(y, x, strata, weight) { 
    # The coxph program may occassionally fail, and this will kill the C
    #  routine below
    if (any(is.na(x)) || any(is.na(y))) return(NULL)   
    <<btree>>
        
    docount <- function(stime, risk, wts) {
        if (attr(stime, 'type') == 'right') {
            ord <- order(stime[,1], -stime[,2])
            ux <- sort(unique(risk))
            n2 <- length(ux)
            index <- btree(n2)[match(risk[ord], ux)] - 1L
             .Call(Cconcordance1, stime[ord,], 
                   as.double(wts[ord]), 
                   as.integer(index), 
                   as.integer(length(ux)))
        }
        else if (attr(stime, 'type') == "counting") {
            sort.stop <- order(-stime[,2], stime[,3])
            sort.start <- order(-stime[,1])
            ux <- sort(unique(risk))
            n2 <- length(ux)
            index <- btree(n2)[match(risk, ux)] - 1L

            .Call(Cconcordance2, stime, 
                  as.double(wts), 
                  as.integer(index), 
                  as.integer(length(ux)),
                  as.integer(sort.stop-1L), 
                  as.integer(sort.start-1L))
        }
        else stop("Invalid survival type for concordance")
    }
        
    if (missing(weight) || length(weight)==0)
        weight <- rep(1.0, length(x))
    storage.mode(y) <- "double"
    
    if (missing(strata) || length(strata)==0) {
        count <- docount(y, x, weight)
        if (count[1]==0 && count[2]==0) count[5]<-0
        else count[5] <- 2*sqrt(count[5])
        names(count) <- c("concordant", "discordant", "tied.risk", "tied.time",
                          "std(c-d)")
    }
    else {
        strata <- as.factor(strata)
        ustrat <- levels(strata)[table(strata) >0]  #some strata may have 0 obs
        count <- matrix(0., nrow=length(ustrat), ncol=5)
        for (i in 1:length(ustrat)) {
            keep <- which(strata == ustrat[i])
            count[i,] <- docount(y[keep,,drop=F], x[keep], weight[keep])
        }
        
        count[,5] <- 2*sqrt(ifelse(count[,1]+count[,2]==0, 0, count[,5]))
        dimnames(count) <- list(ustrat,  c("concordant", "discordant",
                                           "tied.risk", "tied.time",
                                           "std(c-d)"))
    }
    count
}
@ 
