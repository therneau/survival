\subsection{Concordance}
 The concordance statistic is the most a measure of goodness-of-fit
in survival models.  
In general let $y_i$ and $x_i$ be observed and predicted data values.
A pair of obervations $i$, $j$ is considered condordant if either
$y_i > y_j, x_i > x_j$ or $y_i < y_j, x_i < x_j$.
For a Cox model remember that the predicted survival $\hat y$ is longer if
the risk score $X\beta$ is lower.
The concordance $C$ is the fraction of concordant pairs.

One wrinkle is what to do with ties in either $y$ or $x$.  Such pairs
can be ignored in the count (treated as incomparable), treated as discordant,
or given a score of 1/2.
\begin{itemize}
  \item Kendall's $\tau$-a scores ties as 0.
  \item Kendall's $\tau$-b and the Goodman-Kruskal $\gamma$ ignore ties in 
    either $y$ or $x$.
  \item Somers' $d$ treats ties in $y$ as incomparable, pairs that are tied
    in $x$ (but not $y$) score as 1/2.  The AUC from logistic regression is
    equal to Somers' $d$.
\end{itemize}
All three of the above range from -1 to 1, while $C$ ranges from 0 to 1;
$C = (d +1)/2$.  
For survival data any pairs which cannot be ranked with certainty are
considered incomparable.
For instance $s_i$ is censored at time 10 and $s_j$ is an event (or censor) 
at time 20.  Subject $i$ may or may not survive longer than subject $j$.  
Note that if $s_i$ is censored at time
10 and $s_j$ is an event at time 10 then $s_i > s_j$.  
Observations that are in different strata are also incomparable, 
since the Cox model only compares within strata.

The program creates 4 variables, which are the number of concordant pairs, 
discordant, tied on time, and tied on $x$ but not on time.  
The default concordance is 
based on the AUC definition, but all 4 values are reported back so that a user
can recreate the others if desired.

Here is the main routine.
<<concordance>>=
concordance <- function(x, ...) 
    UseMethod("concordance")

concordance.formula <- function(formula, data,
                                weights, subset, na.action,
                                ymin=NULL, ymax=NULL, 
                                timewt=c("S", "n", "S/G", "n/G", "n/G2"),
                                influence=FALSE) {
    Call <- match.call()  # save a copy of of the call, as documentation
    timewt <- match.arg(timewt)
    
    index <- match(c("formula", "data", "weights", "subset", "na.action", "id"),
                   names(Call), nomatch=0)
    temp <- Call[c(1, index)]
    temp[[1L]] <-  quote(stats::model.frame)
    special <- c("strata", "cluster")
    temp$formula <- if(missing(data)) terms(formula, special)
                    else              terms(formula, special, data=data)
    mf <- eval(temp, parent.frame())  # model frame
    if (nrow(mf) ==0) stop("No (non-missing) observations")
    Terms <- terms(mf)

    Y <- model.extract(mf, "response")
    if (!inherits(Y, "Surv")) {
        if (is.numeric(Y) && is.vector(Y))  Y <- Surv(Y)
        else stop("left hand side of the formula  must be a numeric vector or a surival")
    }
    n <- nrow(Y)

    wt <- model.extract(m, 'weights')
    offset<- attr(Terms, "offset")
    if (length(offset)>0) stop("Offset terms not allowed")

    stemp <- untangle.specials(Terms, 'strata')
    if (length(stemp$vars)) {
	if (length(stemp$vars)==1) strat <- m[[stemp$vars]]
	else strat <- strata(m[,stemp$vars], shortlabel=TRUE)
        Terms <- Terms[-stemp$terms]
    }
    else strat <- NULL
    
    cluster<- attr(Terms, "specials")$cluster
    if (length(cluster)) {
        tempc <- untangle.specials(Terms, 'cluster', 1:10)
        ord <- attr(Terms, 'order')[tempc$terms]
        if (any(ord>1)) stop ("Cluster can not be used in an interaction")
        cluster <- strata(mf[,tempc$vars], shortlabel=TRUE)  #allow multiples
        Terms <- Terms[-tempc$terms]  # toss it away
    }
                                            
                                            
    x <- model.matrix(Terms, m)[,-1, drop=FALSE]  #remove the intercept
    if (ncol(x) > 1) stop("Only one predictor variable allowed")

    if (!is.null(ymin) & (length(ymin)> 1 || !is.numeric(ymin)))
        stop("ymin must be a single number")
    if (!is.null(ymax) & (length(ymax)> 1 || !is.numeric(ymax)))
        stop("ymax must be a single number")
    
    count <- concordance.fit(Y, x, strat, wt, ymin, ymax, timewt, influence)
    if (is.null(strat)) {
        concordance <- (count[1] + count[3]/2)/sum(count[1:3])
        std.err <- count[5]/(2* sum(count[1:3]))
        }
    else {
        temp <- colSums(count)
        concordance <- (temp[1] + temp[3]/2)/ sum(temp[1:3])
        std.err <- temp[5]/(2*sum(temp[1:3]))
        }

    fit <- list(concordance= concordance, stats=count, n=n, 
                std.err=std.err, call=Call)
    na.action <- attr(m, "na.action")
    if (length(na.action)) fit$na.action <- na.action

    class(fit) <- 'concordance'
    fit
}

print.concordance <- function(x, ...) {
    if(!is.null(cl <- x$call)) {
        cat("Call:\n")
        dput(cl)
        cat("\n")
        }
    omit <- x$na.action
    if(length(omit))
        cat("  n=", x$n, " (", naprint(omit), ")\n", sep = "")
    else cat("  n=", x$n, "\n")
    cat("Concordance= ", format(x$concordance), " se= ", format(x$std.err),
        '\n', sep='')
    print(x$stats)

    invisible(x)
    }
@ 

The concordance.fit function is broken out separately, since it is called
by the \code{coxph} routine.
If $y$ is not a survival quantity, then all of the options for the
\code{timewt} parameter lead to the same result, so use the simplest option.

<<concordancefit>>=
concordance.fit <- function(y, x, strata, weight, ymin, ymax, timewt,
                            influence) {
    # The coxph program may occassionally fail, and this will kill the C
    #  routine below
    if (any(is.na(x)) || any(is.na(y))) return(NULL)

    # these should only occur if something outside survival calls this routine
    n <- length(y)
    if (length(x) != n) stop("x and y are not the same length")
    if (missing(strata) || is.null(strata)) strata <- rep(0L, n)
    else if (length(strata) != n)
        stop("y and strata are not the same length")
    if (length(weight) != n) stop("y and weight are not the same length")

    # sort y and x, and get the weights
    if (!is.Surv(y)) {
        if (any(diff(order(strata, y)) < 1)) {
            ord <- order(strata, y)  # y within strata
            y <- y[ord]
            x <- x[ord]
            wt <- wt[ord]
            strata <- strata[ord]
            sfit <- survfitKM(strata, Surv(y), wt, se.fit=FALSE) 
            timewt <- "S"
        }
    }
    else {
        sfit <- survfitKM(strata, y, wt, se.fit=FALSE)
        if (timewt %in% c("S/G", "n/G", "n/G2")) {
            y2 <-y
            y2[,ncol(y)] <- 1- y[,ncol(y)]
            gfit <- survfitKM(strata, y2, wt, se.fit=FALSE)
        }
        if (attr(stime, "type") %in% c("counting", "mcounting")) {
            sort.stop <- order(strata, -stime[,2], stime[,3])
            sort.start <- order(strata, -stime[,1])
        } else {
            ord <- order(strata, y[,1], -y[,2])  # death before censoring
            y <- y[ord,]
            x <- x[ord]
            wt <- wt[ord]
            strata <- strata[ord]
        }
    }
     
    # weights involve S(t) and G(t-), so we need to interpolate G per
    #  stratum
    nstrat <- max(stratum)
    if (timewt %in% c("S/G", "n/G", "n/G2")) {
        gwt <- unlist(lapply(1:nstrat, function(i) {
            stemp <- sfit[i]
            gtemp <- gfit[i]
            etime <- stemp$time[stemp$nevent>0]
            e2 <- etime - min(diff(etime))/2
            summary(gfit[i], times=e2, extend=TRUE)$surv
        }))
    }
    etime <- sfit$time[sfit$nevent>0]
    swt <- sfit$surv[sfit$nevent>0]
    nwt <- (sfit$nrisk- sfit$nevent)[sfit$nevent>0]
    twt <- switch(timewt) {
        "S" = swt
        "S/G" = swt/gwt,
        "n" = nwt,
        "n/G"= nwt/gwt,
        "n/G2" = nwt/gwt^2
    }
    
    # match each score to the unique set (to deal with ties)
    uindex <- lapply(1:nstrat, function(i) {
        temp <- x[strata==i]
        match(temp, sort(unique(temp)))})
    nindx <- sapply(uindex, max)   # number of unique values

    # create the indexing vectors
    <<btree>>
    temp <- btree(max(nindx))
    parent <- as.integer(temp/2L)  # node number of parent
    rchild <- as.integer(temp %%2) # is this node the right child? 
                  
    parent <- unlist(lapply(unidex, function(i) parent[i]))
    rchild <- unlist(lapply(unidex, function(i) rchild[i]))             
  
    if (ncol(y) ==3) 
        counts <- .Call(Cconcordance3, stime, x, wts,
                        parent, child)
    else counts <- .Call(Ccondordanc4, stime, x, wts, sort.start, sort.stop,
                         parent, child)
    counts
}
@ 

The C code looks a lot like a Cox model: walk forward through time, keep
track of the risk sets, and add something to the totals at each death.
What needs to be summed is the rank of the event subject's $x$ value, as
compared to the value for all others at risk at this time point.
For notational simplicity let $Y_j(t_i)$ be an indicator that subject $j$
is at risk at event time $t_i$, and $Y^*_j(t_i)$ the more restrictive one that
subject $j$ is both at risk and not a tied event time.
The values we want at time $t_i$ are
\begin{align}
  n_i &=  \sum_j w_j Y^*_j(t_i) \nonumber
  C_i &= (v_i/n_i) \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i < x_j) 
    \label{C} \\
  D_i &= (v_i/n_i) \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i > x_j) 
     \label{D} \\
  T_i &= (v_i/n_i) \delta_i w_i \sum_j w_j Y^*_j(t_i) \left[I(x_i = x_j) 
     \label{T}  \\
  m_i &= \delta_i \sum_j w_j (Y_j - Y^*_j) \nonumber
\end{align*} 

In the above $n$ is the number of comparable values at event time $t_i$ and
$m$ is the number of exact ties, $(v_i/n_i)$ is treated as a fixed 
time-dependent weight (with no censoring it is a constant).
$C$, $D$, and $T$ are the number of concordant, discordant, and tied
pairs, respectively.

The primary compuational question is how to do this efficiently, i.e., better
than a naive algorithm that loops across all $n(n-1)/2$ 
possible pairs.
There are two key ideas.
\begin{enumerate}
\item Rearrange the counting so that we do it by death times.
  For each death we count the number of other subjects in the risk set whose
  score is higher, lower, or tied and add it into the totals.
This neatly solves the question of time-dependent covariates.
\item Counting the number with higher, lower, and tied $x$ can be don in 
   $O(\log_2 n)$ time if the $x$ data is kept in a binary tree.
\end{enumerate}

\begin{figure}
  \myfig{balance}
  \caption{A balanced tree of 13 nodes.}
  \label{treefig}
\end{figure}

Figure  \ref{treefig} shows a balanced binary tree containing  
13 risk scores.  For each node the left child and all its descendants
have a smaller value than the parent, the right child and all its
descendents have a larger value.
Each node in figure \ref{treefig} is also annotated with the total weight
of observations in that node and the weight for all its left and right children 
(not shown on graph).  
Assume that the tree shown represents all of the subjects still alive at the
time a particular subject ``Smith'' expires, and that Smith has the risk score
of 19 in the tree.
The concordant pairs are those with a risk score $>19$, i.e., both $\hat y$
and $y$ are larger, discordant are $<19$, and we have no ties.
The totals can be found by
\begin{enumerate}
  \item Initialize the counts for discordant, concordant and tied to the
    values from the left children, right children, and ties at this node,
    respectively, which will be $(C,D,T) = (1,1,0)$.
  \item Walk up the tree, and at each step add the (parent + left child) or
    (parent + right child) to either D or C, depending on what part of the
    tree has not yet been totaled.  
    At the next node (8) $D= D+4$, and at the top node $C=C + 6$.
\end{itemize}

There are 5 concordant and 7 discordant pairs.
This takes a little less than $\log_2(n)$ steps on average, as compared to an
average of $n/2$ for the naive method.  The difference can matter when $n$ is
large since this traversal must be done for each event.

The classic way to store trees is as a linked list.  There are several 
algorithms for adding and subtracting nodes from a tree while maintaining
the balance (red-black trees, AA trees, etc) but we take a different 
approach.  Since we need to deal with case weights in the model and we
know all the risk score at the outset, the full set of risk scores is
organised into a tree at the beginning, updating the sums of weights at
each node as observations are added or removed from the risk set.

If we internally index the nodes of the tree as 1 for the top, 
2--3 for the next 
horizontal row, 4--7 for the next, \ldots then the parent-child 
traversal becomes particularly easy.
The parent of node $i$ is $i/2$ (integer arithmetic) and the children of
node $i$ are $2i$ and $2i +1$.  In C code the indices start at 0 of course.
The following bit of code arranges data into such a tree.
<<btree>>=
btree <- function(n, id=1L) {
    if (n==1) id
    else if (n==2) c(2L*id, id)
    else if (n==3) c(2L*id, id, 1L*id + 2L)
    else {
        split <- ceiling(n/2)
        c(btree(split-1, 2L*id), id, btree(n-split, 1L + 2L*id))
    }
}
@ 

Referring again to figure \ref{treefig}, \code{btree(13)} yields the vector
\code{ 8 4 2 10 5 7 1 12 6 3 14 7 9}
meaning that the smallest element
will be in position 8 of the tree, the next smallest in position 2, etc.
This function does not fill the bottom row of the tree from left to 
right, which is okay;
what is important in the algorithm is that the parent of each node has an
index which is $\le n$.

The next question is how to compute a variance for the result.
One approach is to compute an infinitesimal jackknife (IJ) estimate,
for which we need derivatives with respect to the weights.
Looking back at equation \eqref{C} we have
\begin{align}
  C  &= \sum_i (v_i/m_i) w_i \delta_i \sum_j Y^*__j(t_i) w_j I(x_i < x_j) 
  \nonumber\\
 \frac{\partial C}{\partial w_k} &= 
    (v_k/m_k)\delta_k \sum_j Y^*_{j}(t_k) I(x_k < x_j) +
    \sum_i (v_i/m_i) w_i Y^*_k(t_i) I(x_i < x_k) \label{partialC}
\end{align*}
The first term of this is a sum over all subjects who were at risk when subject
$k$ had an event, if any, and the second over all other subjects' events for
which subject $k$ was at risk, in both cases not counting a pair of tied death 
times as comparable to each other.

I had originally avoided the IJ approach because it appears to be an $O(nd)$
summation, adding a bit to each at-risk subject each time there is an
event.  A key insight due to David Watson is that we can avoid this.
Within the tree we keep the sum of weights for each node and for 
its left and right children, along with a parallel triple that totals only 
the deaths.
The basic algorithm is to move through an outer and inner loop.  The
outer loop moves across unique death times, the inner for all obs that
share a time.  Move from largest
to smallest time.
\begin{enumerate}
  \item Move to a new event time
    \item For all data rows which leave the risk set at this time, 
      add the current deaths triple to their IJ estimate (only applicable
      to start, stop data).
    \item For all subjects with an endpoint at this time
    \begin{enumerate}
      \item Add -1 times the deaths triple to the subject's IJ estimate
      \item If not an event, add the subject to the first total in the tree
      \item If an event, add the first triple to their IJ estimate, and also
        increment the global totals for $C$, $D$, and $T$.
    \end{enumerate}
  \item Go through the deaths at this event time again,
     adding their totals to both the overall and death triples in the tree
  \item At the end of each stratum, add the current deaths triple to all
    subjects still in the stratum.
\end{enumerate}

A second variance estimate is based on the Cox model.  
The insight used here is to consider a Cox model with time dependent
covariates, where the covariate $x$ at each death time has been
transformed into ${\rm rank}(x)$. 
It is easy to show that the Cox score statistic contribution at each death
is $(D-C)/2$ where $C$ and $D$ are the number of concordant
and discordant pairs contributed at that death time (for a Cox fit using the
Breslow approximation).
The contribution to the variance of the score statistic is 
$V(t) =\sum (r_i - \overline{r})^2 /n$, the $r_i$ being the ranks at
that time point and $n$ the number at risk.  
How can we update this sum using an update formula?  
First remember the identity
\begin{equation*} \sum w_i(x_i - \overline{x})^2 = \sum w_i(x_i-c)^2 - 
    \sum w_i(c - \overline{x})^2
\end{equation*}
true for any set of values $x$ and centering constant $c$.
For weighted data define the rank of an observation with risk score $r_k$
as
\begin{equation*}
  {\rm rank} = \sum_{r_i<r_k} w_i + (1/2)\sum_{r_i=r_k} w_i
\end{equation*}
These correspond to the midpoints of the rise on an empirical CDF, and
for unweighted data without ties gives ranks of .5, 1.5, \ldots, $n-.5$.
   
Assume we have just added obs $k$ to the tree.
Since the mean rank = $\mu_g = \sum(w_i)/2$ the grand mean increases by
    $w_k/2$.
We can divide the subjects currently in the tree into 3 groups.
\begin{enumerate}
  \item Those with risk score lower than the new addition.  Their rank will
    not change.
  \item Those tied with the new addition.  Their rank will increase by
    $w_k/2$.
  \item Those above the new addition.  Their rank increases by $w_k$.
\end{enumerate}


Let $\mu_\ell$ be the mean rank for all observations currently in the
tree of rank lower than $r_k$, the item we are about to add,
$\mu_u$ be the mean for all those above in rank (after the addition), 
$\mu_g$ the grand mean, and $\mu_n$ the new grand mean after adding
in subject $k$.
We have 
\begin{align*}
  \mu_\ell &= \sum_{r_i<r_k} w_i/2 \\
  \mu_u & =  \sum_{r_i<=r_k} w_i + \sum_{r_i \ge r_k} w_i/2
\end{align*}  

For items of lower rank than $r_k$, none of their ranks will change
with the addition of this new observation.  This leads to the 
update formula on the third line below.  (I'm using $i<k$ as shorthand  %'
for $r_i < r_k$ below)
\begin{align}
   \sum_{i<k} w_i(r_i - \mu_g)^2 &= \sum_{i<k} w_i(r_i - \mu_\ell)^2 +
      (\sum_{i<k} w_i)(\mu_\ell - \mu_g)^2 \nonumber \\
   \sum_{i<k} w_i(r_i - \mu_n)^2 &= \sum_{i<k} w_i(r_i - \mu_\ell)^2 +
      (\sum_{i<k} w_i)(\mu_\ell - \mu_n)^2 \nonumber \\
   \sum_{i<k} w_i(r_i - \mu_n)^2 -   \sum_{i<k} w_i(r_i - \mu_g)^2 &=
    (\sum_{i<k} w_i) [(\mu_\ell - \mu_n)^2 - (\mu_\ell - \mu_g)^2] 
              \nonumber \\
   &=  (\sum_{i<k} w_i)(\mu_n+ \mu_g - 2\mu_{\ell})(\mu_n - \mu_g) 
                    \label{lower1} \\
   &=  (\sum_{i<k} w_i)(\mu_n+ \mu_g - 2\mu_{\ell}) w_k/2 \label{lower}
\end{align}

For items of larger rank than $r_k$, all of the ranks increase by $w_k$
when we add the new item and $\mu_u$ increases by $w_k$, 
thus the sum of squares within the group
is unchanged.  The same derivation as above gives an update of
\begin{align}
  \sum_{i>k} w_i(r_i - \mu_n)^2 -   \sum_{i>k} w_i(r_i - \mu_g)^2 &=
    (\sum_{i>k} w_i) [(\mu_u -\mu_n)^2 - ((\mu_u-w_k) - \mu_g)^2] \nonumber \\
  &= (\sum_{i>k} w_i) (\mu_n + z - 2\mu_u)(\mu_n -z) \label{upper1} \\
  &= (\sum_{i>k} w_i) (\mu_n+z - 2\mu_u) (-w_k/2) \label{upper}\\
  z&\equiv \mu_g+ w_k \nonumber
\end{align}

For items of tied rank, their rank increases by the same amount as the
overall mean, and so their contribution to the total SS is unchanged.
The final part of the update step is to add in the SS contributed by
the new observation.

An observation is removed from the tree whenver the current time becomes
less than the (start, stop] interval of the datum.
The ranks for observations of lower risk are unchanged by the removal
so equation \eqref{lower1} applies just as before, but with the new mean
smaller than the old so the last term in equation \eqref{lower} changes
sign.
For the observations of higher risk both the mean and the ranks change
by $w_k$ and equation \eqref{upper1} holds but with $z=\mu_0- w_k$.


We can now define the C-routine that does the bulk of the work.
First we give the outline shell of the code and then discuss the
parts one by one.  This routine  is for ordinary survival data, and
will be called once per stratum.
Input variables are
\begin{description}
  \item[n] the number of observations
  \item[y] matrix containing the time and status, data is sorted by ascending 
    time, with deaths preceding censorings.
  \item[indx] the tree node at which this observation's risk score resides  %'
  \item[wt] case weight for the observation
  \item[sum] scratch space, weights for each node of the tree: 
    3 values are for the node, all left children, and all right children
  \item[count] the returned counts of concordant, discordant, tied on x, 
    tied on time, and the variance
\end{description}

<<concordance1>>=
#include "survS.h"
SEXP concordance1(SEXP y, SEXP wt2,  SEXP indx2, SEXP ntree2) {
    int i, j, k, index;
    int child, parent;
    int n, ntree;
    double *time, *status;
    double *twt, *nwt, *count;
    double vss, myrank, wsum1, wsum2, wsum3; /*sum of wts below, tied, above*/
    double lmean, umean, oldmean, newmean;
        
    double ndeath;   /* weighted number of deaths at this point */
    
    SEXP count2;
    double *wt;
    int    *indx;
    
    n = nrows(y);
    ntree = asInteger(ntree2);
    wt = REAL(wt2);
    indx = INTEGER(indx2);
    
    time = REAL(y);
    status = time + n;
    PROTECT(count2 = allocVector(REALSXP, 5));
    count = REAL(count2);  /* count5 contains the information matrix */
    twt = (double *) R_alloc(2*ntree, sizeof(double));
    nwt = twt + ntree;
    for (i=0; i< 2*ntree; i++) twt[i] =0.0;
    for (i=0; i<5; i++) count[i]=0.0;
    vss=0;

    <<concordance1-work>>
        
    UNPROTECT(1);
    return(count2);
}
@ 
The key part of our computation is to update the vectors of weights.
We don't actually pass the risk score values $r$ into the routine,   %'
it is enough for each observation to point to the appropriate tree
node.
The tree contains the  for everyone whose survival is larger
than the time currently under review, so starts with all weights
equal to zero.  
For any pair of observations $i,j$ we need to add [[wt[i]*wt[j]]]
to the appropriate count.
Starting at the largest time (which is sorted last), walk through the tree.
\begin{itemize}
  \item If it is a death time, we need to process all the deaths tied at
    this time.
    \begin{enumerate}
      \item Add [[wt[i] * wt[j]]] to the tied-on-time total, 
	for all pairs $i,j$ of tied times.
      \item The addition to tied-on-r will be the weight of this 
        observation times
        the sum of weights for all others with the same risk score and a
        a greater time, i.e., the weight found at [[indx[i]]] in the tree.
      \item Similarly for those with smaller or larger risk scores.  First add
        in the children of this node.  The left child will be smaller risk 
        scores (and longer times) adding to the concordant pairs, 
        the right child discordant.
        Then walk up the tree to the root. 
        At each step up we add in data for the 'not me' branch.
        If we were the right branch (even number node) of a parent
        then when moving up we add in the left branch counts, and vice-versa. 
    \end{enumerate}
    \item Now add this set of subject weights into the tree. The weight for
      a node is [[nwt]] and for the node and all its children is [[twt]].
\end{itemize}
<<concordance1-work>>=
for (i=n-1; i>=0; ) {
    ndeath =0;
    if (status[i]==1) { /* process all tied deaths at this point */
        for (j=i; j>=0 && status[j]==1 && time[j]==time[i]; j--) {
            ndeath += wt[j];
            index = indx[j];
            for (k=i; k>j; k--) count[3] += wt[j]*wt[k]; /* tied on time */
            count[2] += wt[j] * nwt[index];              /* tied on x */
	    child = (2*index) +1;  /* left child */
	    if (child < ntree)
		count[0] += wt[j] * twt[child];  /*left children */
	    child++;
	    if (child < ntree)
                count[1] += wt[j] * twt[child]; /*right children */
	    
            while (index >0) {  /* walk up the tree  */
		parent = (index-1)/2;
		if (index & 1)   /* I am the left child */
		    count[1] += wt[j] * (twt[parent] - twt[index]);
		else count[0] += wt[j] * (twt[parent] - twt[index]);
		index = parent;
		}
	    }
	}                    
    else j = i-1;
    
    /* Add the weights for these obs into the tree and update variance*/
    for (; i>j; i--) {
        wsum1=0; 
        oldmean = twt[0]/2;
	index = indx[i];
	nwt[index] += wt[i];
        twt[index] += wt[i];
        wsum2 = nwt[index];
        child = 2*index +1;  /* left child */
        if (child < ntree) wsum1 += twt[child];

	while (index >0) {
	    parent = (index-1)/2;
            twt[parent] += wt[i];
            if (!(index&1)) /* I am a right child */
                wsum1 += (twt[parent] - twt[index]);
            index=parent;
            }
        wsum3 = twt[0] - (wsum1 + wsum2); /* sum of weights above */
        lmean = wsum1/2;
        umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
        newmean = twt[0]/2;
        myrank = wsum1 + wsum2/2;
        vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean - oldmean);
        vss += wsum3*(newmean+ oldmean+ wt[i]- 2*umean) *(oldmean-newmean);
        vss += wt[i]* (myrank -newmean)*(myrank -newmean);
	}
    count[4] += ndeath * vss/twt[0];
    }
@ 

The code for [start, stop) data is quite similar.  
As in the agreg routines there are two sort indices, the first indexes
the data by stop time, longest to earliest, and the second by start time. 
The [[y]] variable now has three columns.
<<concordance1>>= 
SEXP concordance2(SEXP y,     SEXP wt2,  SEXP indx2, SEXP ntree2,
                  SEXP sortstop, SEXP sortstart) {
    int i, j, k, index;
    int child, parent;
    int n, ntree;
    int istart, iptr, jptr;
    double *time1, *time2, *status, dtime;
    double *twt, *nwt, *count;
    int *sort1, *sort2;
    double vss, myrank;
    double wsum1, wsum2, wsum3; /*sum of wts below, tied, above*/
    double lmean, umean, oldmean, newmean;
    double ndeath;
    SEXP count2;
    double *wt;
    int    *indx;
    
    n = nrows(y);
    ntree = asInteger(ntree2);
    wt = REAL(wt2);
    indx = INTEGER(indx2);
    sort2 = INTEGER(sortstop);
    sort1 = INTEGER(sortstart);
    
    time1 = REAL(y);
    time2 = time1 + n;
    status= time2 + n;
    PROTECT(count2 = allocVector(REALSXP, 5));
    count = REAL(count2);
    twt = (double *) R_alloc(2*ntree, sizeof(double));
    nwt = twt + ntree;
    for (i=0; i< 2*ntree; i++) twt[i] =0.0;
    for (i=0; i<5; i++) count[i]=0.0;
    vss =0;
    <<concordance2-work>>
        
    UNPROTECT(1);
    return(count2);
}
@ 

The processing changes in 2 ways
\begin{itemize}
  \item The loops go from $0$ to $n-1$ instead of $n-1$ to 0.  We need
    to use [[sort1[i]]] instead of [[i]] as the subscript for the time2 and wt
    vectors.  (The sort vectors go backwards in time.)
    This happens enough that we use a temporary variables [[iptr]] and [[jptr]]
    to avoid the double subscript.
  \item As we move from the longest time to the shortest observations are added
    into the tree of weights whenever we encounter their stop time. 
    This is just as before.  Weights now also need to be removed from the 
    tree whenever we encounter an observation's start time.              %'
    It is convenient ``catch up'' on this second task whenever we encounter 
    a death.
\end{itemize}

<<concordance2-work>>=
istart = 0;  /* where we are with start times */
for (i=0; i<n; ) {
    iptr = sort2[i];  /* In  reverse death time order */
    ndeath =0;
    if (status[iptr]==1) {
	/* Toss people out of the tree  and update variance */
	dtime = time2[iptr];
	for (; istart < n && time1[sort1[istart]] >= dtime; istart++) {
            wsum1 =0;
	    oldmean = twt[0]/2;
	    jptr = sort1[istart];
	    index = indx[jptr];
	    nwt[index] -= wt[jptr];
            twt[index] -= wt[jptr];
            wsum2 = nwt[index];
            child = 2*index +1;  /* left child */
	    if (child < ntree) wsum1 += twt[child];
	    while (index >0) {
		parent = (index-1)/2;
		twt[parent] -= wt[jptr];
		if (!(index&1)) /* I am a right child */
		    wsum1 += (twt[parent] - twt[index]);
		index=parent;
		}
	    wsum3 = twt[0] - (wsum1 + wsum2);
	    lmean = wsum1/2;
	    umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
	    newmean = twt[0]/2;
	    myrank = wsum1 + wsum2/2;
            vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean-oldmean);
            oldmean -= wt[jptr];  /* the z in equations above */
            vss += wsum3*(newmean+ oldmean -2*umean) * (newmean-oldmean);
	    vss -= wt[jptr]* (myrank -newmean)*(myrank -newmean);
            }
	    
	/* Process deaths */
	for (j=i; j <n && status[sort2[j]]==1 && time2[sort2[j]]==dtime; j++) {
	    jptr =  sort2[j];
	    ndeath += wt[jptr];
            index = indx[jptr];
            for (k=i; k<j; k++) count[3] += wt[jptr]*wt[sort2[k]]; 
            count[2] += wt[jptr] * nwt[index];            /* tied on x */
            child = (2*index) +1;   /* left child */
            if (child < ntree) count[0] += wt[jptr] * twt[child];
            child++;
            if (child < ntree) count[1] += wt[jptr] * twt[child];

            while (index >0) {  /* walk up the tree  */
                parent = (index-1)/2;
                if (index &1)   /* I am the left child */
                     count[1] += wt[jptr] * (twt[parent] - twt[index]);
                else count[0] += wt[jptr] * (twt[parent] - twt[index]);
                index = parent;
                }
	    }                    
        }
    else j = i+1;

    /* Add the weights for these obs into the tree and compute variance */
    for (; i<j; i++) {
        wsum1 =0;
        oldmean = twt[0]/2;
        iptr = sort2[i];
	index = indx[iptr];
        nwt[index] += wt[iptr];
	twt[index] += wt[iptr];
        wsum2 = nwt[index];
        child = 2*index +1;  /* left child */
        if (child < ntree) wsum1 += twt[child];
	while (index >0) {
            parent = (index-1)/2;
            twt[parent] += wt[iptr];
            if (!(index&1)) /* I am a right child */
                wsum1 += (twt[parent] - twt[index]);
            index=parent;
	    }
        wsum3 = twt[0] - (wsum1 + wsum2);
        lmean = wsum1/2;
        umean = wsum1 + wsum2 + wsum3/2;  /* new upper mean */
        newmean = twt[0]/2;
        myrank = wsum1 + wsum2/2;
        vss += wsum1*(newmean+ oldmean - 2*lmean) * (newmean-oldmean);
        vss += wsum3*(newmean+ oldmean +wt[iptr] - 2*umean) * (oldmean-newmean);
        vss += wt[iptr]* (myrank -newmean)*(myrank -newmean);
        }
    count[4] += ndeath * vss/twt[0];
    }
@ 

One last wrinkle is tied risk scores: they are all set to point to
the same node of the tree.

This part of the compuation is a separate function, since it is
also called by the coxph routines. 
Although we are very careful to create integers and/or doubles for the
arguments to .Call I still wrap them in the appropriate as.xxx 
construction: ``belt and suspenders''.
Also, referring to the the mathematics many paragraphs ago, the C routine
returns the variance of $(C-D)/2$ and we return the standard deviation of
$(C-D)$.
If this routine is called with all the x values identical, then $C$ and $D$
will both be zero, but the calculated variance of $C-D$ can be a nonzero
tiny number due to round off error.  Since this can cause a warning message
from the sqrt function we check and correct this.
<<survConcordance.fit>>=
survConcordance.fit <- function(y, x, strata, weight) { 
    # The coxph program may occassionally fail, and this will kill the C
    #  routine below
    if (any(is.na(x)) || any(is.na(y))) return(NULL)   
    <<btree>>
        
    docount <- function(stime, risk, wts) {
        if (attr(stime, 'type') == 'right') {
            ord <- order(stime[,1], -stime[,2])
            ux <- sort(unique(risk))
            n2 <- length(ux)
            index <- btree(n2)[match(risk[ord], ux)] - 1L
             .Call(Cconcordance1, stime[ord,], 
                   as.double(wts[ord]), 
                   as.integer(index), 
                   as.integer(length(ux)))
        }
        else if (attr(stime, 'type') == "counting") {
            sort.stop <- order(-stime[,2], stime[,3])
            sort.start <- order(-stime[,1])
            ux <- sort(unique(risk))
            n2 <- length(ux)
            index <- btree(n2)[match(risk, ux)] - 1L

            .Call(Cconcordance2, stime, 
                  as.double(wts), 
                  as.integer(index), 
                  as.integer(length(ux)),
                  as.integer(sort.stop-1L), 
                  as.integer(sort.start-1L))
        }
        else stop("Invalid survival type for concordance")
    }
        
    if (missing(weight) || length(weight)==0)
        weight <- rep(1.0, length(x))
    storage.mode(y) <- "double"
    
    if (missing(strata) || length(strata)==0) {
        count <- docount(y, x, weight)
        if (count[1]==0 && count[2]==0) count[5]<-0
        else count[5] <- 2*sqrt(count[5])
        names(count) <- c("concordant", "discordant", "tied.risk", "tied.time",
                          "std(c-d)")
    }
    else {
        strata <- as.factor(strata)
        ustrat <- levels(strata)[table(strata) >0]  #some strata may have 0 obs
        count <- matrix(0., nrow=length(ustrat), ncol=5)
        for (i in 1:length(ustrat)) {
            keep <- which(strata == ustrat[i])
            count[i,] <- docount(y[keep,,drop=F], x[keep], weight[keep])
        }
        
        count[,5] <- 2*sqrt(ifelse(count[,1]+count[,2]==0, 0, count[,5]))
        dimnames(count) <- list(ustrat,  c("concordant", "discordant",
                                           "tied.risk", "tied.time",
                                           "std(c-d)"))
    }
    count
}
@ 
